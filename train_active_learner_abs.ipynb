{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 31\n",
    "seed_everything(SEED)\n",
    "\n",
    "import flair\n",
    "flair.device = torch.device('cuda:2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oracle import HybridOracle\n",
    "from active_learner import ActiveLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'absolute_sampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../twitter_dataset/train_10.csv\n",
      "Starting with labelled data\n"
     ]
    }
   ],
   "source": [
    "oracle = HybridOracle(\n",
    "    experiment_name=EXPERIMENT,\n",
    "    all_data_file='../twitter_dataset/train_10.csv',\n",
    "    valid_file='../twitter_dataset/valid.csv',\n",
    "    test_file='../twitter_dataset/test.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = oracle.get_all_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ActiveLearner(sentences,\n",
    "                        experiment_name=EXPERIMENT,\n",
    "                        oracle=oracle, \n",
    "                        embeddings_storage_mode='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Dictionary, Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "                    FlairEmbeddings('news-forward'),\n",
    "                    FlairEmbeddings('news-backward'),\n",
    "                   ]\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )\n",
    "\n",
    "label_dict = Dictionary()\n",
    "label_dict.idx2item = [b'0', b'1']\n",
    "label_dict.item2idx = {b'0': 0, b'1': 1}\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:35:30,051 loading file ../twitter_dataset/0_1/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.load('../twitter_dataset/0_1/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:36:38,554 Reading data from absolute_sampling\n",
      "2019-10-19 13:36:38,555 Train: absolute_sampling/labelled_128_1571492198.csv\n",
      "2019-10-19 13:36:38,557 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 13:36:38,558 Test: absolute_sampling/test.csv\n",
      "2019-10-19 13:36:38,646 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:38,648 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:36:38,650 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:38,651 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:36:38,651 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:38,652 Parameters:\n",
      "2019-10-19 13:36:38,653  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:36:38,653  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:36:38,654  - patience: \"5\"\n",
      "2019-10-19 13:36:38,655  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:36:38,655  - max_epochs: \"1\"\n",
      "2019-10-19 13:36:38,656  - shuffle: \"True\"\n",
      "2019-10-19 13:36:38,656  - train_with_dev: \"False\"\n",
      "2019-10-19 13:36:38,657 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:38,658 Model training base path: \"absolute_sampling/0\"\n",
      "2019-10-19 13:36:38,658 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:38,659 Device: cuda:2\n",
      "2019-10-19 13:36:38,659 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:38,661 Embeddings storage mode: gpu\n",
      "2019-10-19 13:36:38,663 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:39,224 epoch 1 - iter 0/4 - loss 1.14353490 - samples/sec: 141.19\n",
      "2019-10-19 13:36:39,485 epoch 1 - iter 1/4 - loss 0.94395220 - samples/sec: 138.67\n",
      "2019-10-19 13:36:39,719 epoch 1 - iter 2/4 - loss 0.89968805 - samples/sec: 148.47\n",
      "2019-10-19 13:36:39,962 epoch 1 - iter 3/4 - loss 0.87687044 - samples/sec: 144.07\n",
      "2019-10-19 13:36:40,131 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:40,132 EPOCH 1 done: loss 0.8769 - lr 0.0000\n",
      "2019-10-19 13:37:50,033 DEV : loss 0.7225503921508789 - score 0.6898\n",
      "2019-10-19 13:37:52,614 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:37:59,948 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:37:59,949 Testing using best model ...\n",
      "2019-10-19 13:37:59,951 loading file absolute_sampling/0/best-model.pt\n",
      "2019-10-19 13:39:18,442 0.7014\t0.7015\t0.7014\n",
      "2019-10-19 13:39:18,444 \n",
      "MICRO_AVG: acc 0.5402 - f1-score 0.7014\n",
      "MACRO_AVG: acc 0.5399 - f1-score 0.7011499999999999\n",
      "0          tp: 3674 - fp: 1669 - fn: 1316 - tn: 3342 - precision: 0.6876 - recall: 0.7363 - accuracy: 0.5517 - f1-score: 0.7111\n",
      "1          tp: 3341 - fp: 1317 - fn: 1669 - tn: 3674 - precision: 0.7173 - recall: 0.6669 - accuracy: 0.5281 - f1-score: 0.6912\n",
      "2019-10-19 13:39:18,444 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "classifier, opt_state = learner.step(classifier, step_num=0, max_sample_size=100000, labelling_step_size=128, step_lr=1e-5, sampling_method='absolute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:40:23,584 Reading data from absolute_sampling\n",
      "2019-10-19 13:40:23,585 Train: absolute_sampling/labelled_128_1571492423.csv\n",
      "2019-10-19 13:40:23,588 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 13:40:23,589 Test: absolute_sampling/test.csv\n",
      "2019-10-19 13:40:23,664 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:23,666 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:40:23,667 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:23,669 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:40:23,670 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:23,671 Parameters:\n",
      "2019-10-19 13:40:23,671  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:40:23,672  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:40:23,673  - patience: \"5\"\n",
      "2019-10-19 13:40:23,673  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:40:23,674  - max_epochs: \"1\"\n",
      "2019-10-19 13:40:23,674  - shuffle: \"True\"\n",
      "2019-10-19 13:40:23,675  - train_with_dev: \"False\"\n",
      "2019-10-19 13:40:23,676 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:23,676 Model training base path: \"absolute_sampling/1\"\n",
      "2019-10-19 13:40:23,677 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:23,678 Device: cuda:2\n",
      "2019-10-19 13:40:23,678 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:23,679 Embeddings storage mode: gpu\n",
      "2019-10-19 13:40:23,683 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:24,443 epoch 1 - iter 0/4 - loss 0.98324704 - samples/sec: 126.49\n",
      "2019-10-19 13:40:24,681 epoch 1 - iter 1/4 - loss 0.93326420 - samples/sec: 147.17\n",
      "2019-10-19 13:40:24,910 epoch 1 - iter 2/4 - loss 0.89681671 - samples/sec: 150.37\n",
      "2019-10-19 13:40:25,118 epoch 1 - iter 3/4 - loss 0.94402476 - samples/sec: 167.95\n",
      "2019-10-19 13:40:25,347 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:25,349 EPOCH 1 done: loss 0.9440 - lr 0.0000\n",
      "2019-10-19 13:41:42,083 DEV : loss 0.7436191439628601 - score 0.6854\n",
      "2019-10-19 13:41:44,828 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:41:51,373 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:41:51,376 Testing using best model ...\n",
      "2019-10-19 13:41:51,379 loading file absolute_sampling/1/best-model.pt\n",
      "2019-10-19 13:43:02,461 0.6915\t0.6916\t0.6915\n",
      "2019-10-19 13:43:02,462 \n",
      "MICRO_AVG: acc 0.5285 - f1-score 0.6915\n",
      "MACRO_AVG: acc 0.5266 - f1-score 0.68935\n",
      "0          tp: 3881 - fp: 1975 - fn: 1109 - tn: 3036 - precision: 0.6627 - recall: 0.7778 - accuracy: 0.5572 - f1-score: 0.7157\n",
      "1          tp: 3035 - fp: 1110 - fn: 1975 - tn: 3881 - precision: 0.7322 - recall: 0.6058 - accuracy: 0.4959 - f1-score: 0.6630\n",
      "2019-10-19 13:43:02,463 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:44:12,114 Reading data from absolute_sampling\n",
      "2019-10-19 13:44:12,118 Train: absolute_sampling/labelled_128_1571492652.csv\n",
      "2019-10-19 13:44:12,123 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 13:44:12,126 Test: absolute_sampling/test.csv\n",
      "2019-10-19 13:44:12,211 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:12,212 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:44:12,213 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:12,214 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:44:12,214 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:12,215 Parameters:\n",
      "2019-10-19 13:44:12,216  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:44:12,216  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:44:12,217  - patience: \"5\"\n",
      "2019-10-19 13:44:12,217  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:44:12,218  - max_epochs: \"1\"\n",
      "2019-10-19 13:44:12,219  - shuffle: \"True\"\n",
      "2019-10-19 13:44:12,219  - train_with_dev: \"False\"\n",
      "2019-10-19 13:44:12,220 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:12,221 Model training base path: \"absolute_sampling/2\"\n",
      "2019-10-19 13:44:12,221 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:12,225 Device: cuda:2\n",
      "2019-10-19 13:44:12,225 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:12,228 Embeddings storage mode: gpu\n",
      "2019-10-19 13:44:12,232 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:13,022 epoch 1 - iter 0/4 - loss 1.01380682 - samples/sec: 131.64\n",
      "2019-10-19 13:44:13,270 epoch 1 - iter 1/4 - loss 0.87640479 - samples/sec: 143.80\n",
      "2019-10-19 13:44:13,524 epoch 1 - iter 2/4 - loss 0.82395146 - samples/sec: 138.69\n",
      "2019-10-19 13:44:13,792 epoch 1 - iter 3/4 - loss 0.91903715 - samples/sec: 130.64\n",
      "2019-10-19 13:44:14,014 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:14,016 EPOCH 1 done: loss 0.9190 - lr 0.0000\n",
      "2019-10-19 13:45:23,975 DEV : loss 0.7807696461677551 - score 0.676\n",
      "2019-10-19 13:45:27,266 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:45:34,543 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:45:34,544 Testing using best model ...\n",
      "2019-10-19 13:45:34,545 loading file absolute_sampling/2/best-model.pt\n",
      "2019-10-19 13:46:55,146 0.6808\t0.6809\t0.6808\n",
      "2019-10-19 13:46:55,148 \n",
      "MICRO_AVG: acc 0.5161 - f1-score 0.6808\n",
      "MACRO_AVG: acc 0.5109 - f1-score 0.67475\n",
      "0          tp: 4092 - fp: 2293 - fn: 898 - tn: 2718 - precision: 0.6409 - recall: 0.8200 - accuracy: 0.5619 - f1-score: 0.7195\n",
      "1          tp: 2717 - fp: 899 - fn: 2293 - tn: 4092 - precision: 0.7514 - recall: 0.5423 - accuracy: 0.4598 - f1-score: 0.6300\n",
      "2019-10-19 13:46:55,149 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:47:59,299 Reading data from absolute_sampling\n",
      "2019-10-19 13:47:59,300 Train: absolute_sampling/labelled_128_1571492879.csv\n",
      "2019-10-19 13:47:59,307 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 13:47:59,308 Test: absolute_sampling/test.csv\n",
      "2019-10-19 13:47:59,358 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:47:59,359 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:47:59,361 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:47:59,363 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:47:59,366 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:47:59,366 Parameters:\n",
      "2019-10-19 13:47:59,367  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:47:59,368  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:47:59,369  - patience: \"5\"\n",
      "2019-10-19 13:47:59,371  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:47:59,371  - max_epochs: \"1\"\n",
      "2019-10-19 13:47:59,374  - shuffle: \"True\"\n",
      "2019-10-19 13:47:59,376  - train_with_dev: \"False\"\n",
      "2019-10-19 13:47:59,376 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:47:59,377 Model training base path: \"absolute_sampling/3\"\n",
      "2019-10-19 13:47:59,378 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:47:59,379 Device: cuda:2\n",
      "2019-10-19 13:47:59,379 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:47:59,380 Embeddings storage mode: gpu\n",
      "2019-10-19 13:47:59,385 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:00,160 epoch 1 - iter 0/4 - loss 0.88973123 - samples/sec: 120.44\n",
      "2019-10-19 13:48:00,406 epoch 1 - iter 1/4 - loss 0.85094598 - samples/sec: 143.16\n",
      "2019-10-19 13:48:00,642 epoch 1 - iter 2/4 - loss 0.93025186 - samples/sec: 146.50\n",
      "2019-10-19 13:48:00,881 epoch 1 - iter 3/4 - loss 0.91121578 - samples/sec: 146.12\n",
      "2019-10-19 13:48:01,108 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:01,110 EPOCH 1 done: loss 0.9112 - lr 0.0000\n",
      "2019-10-19 13:49:19,751 DEV : loss 0.8327692151069641 - score 0.6633\n",
      "2019-10-19 13:49:23,515 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:49:33,783 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:49:33,784 Testing using best model ...\n",
      "2019-10-19 13:49:33,786 loading file absolute_sampling/3/best-model.pt\n",
      "2019-10-19 13:50:47,205 0.6698\t0.6699\t0.6698\n",
      "2019-10-19 13:50:47,208 \n",
      "MICRO_AVG: acc 0.5036 - f1-score 0.6698\n",
      "MACRO_AVG: acc 0.4941 - f1-score 0.6586000000000001\n",
      "0          tp: 4257 - fp: 2568 - fn: 733 - tn: 2443 - precision: 0.6237 - recall: 0.8531 - accuracy: 0.5632 - f1-score: 0.7206\n",
      "1          tp: 2442 - fp: 734 - fn: 2568 - tn: 4257 - precision: 0.7689 - recall: 0.4874 - accuracy: 0.4251 - f1-score: 0.5966\n",
      "2019-10-19 13:50:47,212 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:51:53,954 Reading data from absolute_sampling\n",
      "2019-10-19 13:51:53,956 Train: absolute_sampling/labelled_128_1571493113.csv\n",
      "2019-10-19 13:51:53,957 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 13:51:53,957 Test: absolute_sampling/test.csv\n",
      "2019-10-19 13:51:54,036 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:54,037 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:51:54,040 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:54,040 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:51:54,041 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:54,042 Parameters:\n",
      "2019-10-19 13:51:54,042  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:51:54,049  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:51:54,050  - patience: \"5\"\n",
      "2019-10-19 13:51:54,053  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:51:54,054  - max_epochs: \"1\"\n",
      "2019-10-19 13:51:54,055  - shuffle: \"True\"\n",
      "2019-10-19 13:51:54,056  - train_with_dev: \"False\"\n",
      "2019-10-19 13:51:54,056 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:54,059 Model training base path: \"absolute_sampling/4\"\n",
      "2019-10-19 13:51:54,059 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:54,060 Device: cuda:2\n",
      "2019-10-19 13:51:54,061 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:54,061 Embeddings storage mode: gpu\n",
      "2019-10-19 13:51:54,065 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:55,047 epoch 1 - iter 0/4 - loss 0.68260390 - samples/sec: 134.53\n",
      "2019-10-19 13:51:55,312 epoch 1 - iter 1/4 - loss 0.78462613 - samples/sec: 136.07\n",
      "2019-10-19 13:51:55,551 epoch 1 - iter 2/4 - loss 0.74546997 - samples/sec: 146.91\n",
      "2019-10-19 13:51:55,791 epoch 1 - iter 3/4 - loss 0.74436578 - samples/sec: 145.76\n",
      "2019-10-19 13:51:56,079 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:51:56,081 EPOCH 1 done: loss 0.7444 - lr 0.0000\n",
      "2019-10-19 13:53:12,946 DEV : loss 0.8984953165054321 - score 0.6441\n",
      "2019-10-19 13:53:15,459 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:53:23,148 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:53:23,149 Testing using best model ...\n",
      "2019-10-19 13:53:23,151 loading file absolute_sampling/4/best-model.pt\n",
      "2019-10-19 13:54:43,803 0.6583\t0.6584\t0.6583\n",
      "2019-10-19 13:54:43,805 \n",
      "MICRO_AVG: acc 0.4907 - f1-score 0.6583\n",
      "MACRO_AVG: acc 0.4755 - f1-score 0.6394\n",
      "0          tp: 4438 - fp: 2864 - fn: 552 - tn: 2147 - precision: 0.6078 - recall: 0.8894 - accuracy: 0.5651 - f1-score: 0.7221\n",
      "1          tp: 2146 - fp: 553 - fn: 2864 - tn: 4438 - precision: 0.7951 - recall: 0.4283 - accuracy: 0.3858 - f1-score: 0.5567\n",
      "2019-10-19 13:54:43,806 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:55:47,427 Reading data from absolute_sampling\n",
      "2019-10-19 13:55:47,428 Train: absolute_sampling/labelled_128_1571493347.csv\n",
      "2019-10-19 13:55:47,437 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 13:55:47,439 Test: absolute_sampling/test.csv\n",
      "2019-10-19 13:55:47,486 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:47,487 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:55:47,488 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:47,488 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:55:47,489 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:47,491 Parameters:\n",
      "2019-10-19 13:55:47,497  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:55:47,498  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:55:47,501  - patience: \"5\"\n",
      "2019-10-19 13:55:47,502  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:55:47,502  - max_epochs: \"1\"\n",
      "2019-10-19 13:55:47,503  - shuffle: \"True\"\n",
      "2019-10-19 13:55:47,504  - train_with_dev: \"False\"\n",
      "2019-10-19 13:55:47,505 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:47,505 Model training base path: \"absolute_sampling/5\"\n",
      "2019-10-19 13:55:47,510 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:47,512 Device: cuda:2\n",
      "2019-10-19 13:55:47,513 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:47,515 Embeddings storage mode: gpu\n",
      "2019-10-19 13:55:47,519 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:48,506 epoch 1 - iter 0/4 - loss 1.08630633 - samples/sec: 137.30\n",
      "2019-10-19 13:55:48,775 epoch 1 - iter 1/4 - loss 0.90905869 - samples/sec: 127.34\n",
      "2019-10-19 13:55:49,023 epoch 1 - iter 2/4 - loss 0.91168402 - samples/sec: 142.99\n",
      "2019-10-19 13:55:49,262 epoch 1 - iter 3/4 - loss 0.87104373 - samples/sec: 150.09\n",
      "2019-10-19 13:55:49,634 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:55:49,635 EPOCH 1 done: loss 0.8710 - lr 0.0000\n",
      "2019-10-19 13:57:00,636 DEV : loss 0.9770041704177856 - score 0.6288\n",
      "2019-10-19 13:57:04,451 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:57:14,137 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:57:14,138 Testing using best model ...\n",
      "2019-10-19 13:57:14,139 loading file absolute_sampling/5/best-model.pt\n",
      "2019-10-19 13:58:35,461 0.6454\t0.6455\t0.6454\n",
      "2019-10-19 13:58:35,463 \n",
      "MICRO_AVG: acc 0.4765 - f1-score 0.6454\n",
      "MACRO_AVG: acc 0.4548 - f1-score 0.6175999999999999\n",
      "0          tp: 4578 - fp: 3133 - fn: 412 - tn: 1878 - precision: 0.5937 - recall: 0.9174 - accuracy: 0.5636 - f1-score: 0.7209\n",
      "1          tp: 1877 - fp: 413 - fn: 3133 - tn: 4578 - precision: 0.8197 - recall: 0.3747 - accuracy: 0.3461 - f1-score: 0.5143\n",
      "2019-10-19 13:58:35,463 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:59:40,423 Reading data from absolute_sampling\n",
      "2019-10-19 13:59:40,425 Train: absolute_sampling/labelled_128_1571493580.csv\n",
      "2019-10-19 13:59:40,426 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 13:59:40,426 Test: absolute_sampling/test.csv\n",
      "2019-10-19 13:59:40,472 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:40,473 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:59:40,474 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:40,475 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:59:40,476 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:40,477 Parameters:\n",
      "2019-10-19 13:59:40,478  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:59:40,478  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:59:40,479  - patience: \"5\"\n",
      "2019-10-19 13:59:40,480  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:59:40,481  - max_epochs: \"1\"\n",
      "2019-10-19 13:59:40,482  - shuffle: \"True\"\n",
      "2019-10-19 13:59:40,482  - train_with_dev: \"False\"\n",
      "2019-10-19 13:59:40,484 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:40,485 Model training base path: \"absolute_sampling/6\"\n",
      "2019-10-19 13:59:40,486 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:40,486 Device: cuda:2\n",
      "2019-10-19 13:59:40,487 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:40,487 Embeddings storage mode: gpu\n",
      "2019-10-19 13:59:40,490 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:41,278 epoch 1 - iter 0/4 - loss 0.84023070 - samples/sec: 134.09\n",
      "2019-10-19 13:59:41,519 epoch 1 - iter 1/4 - loss 0.80792770 - samples/sec: 148.64\n",
      "2019-10-19 13:59:41,751 epoch 1 - iter 2/4 - loss 0.78142277 - samples/sec: 152.64\n",
      "2019-10-19 13:59:42,003 epoch 1 - iter 3/4 - loss 0.74505726 - samples/sec: 142.76\n",
      "2019-10-19 13:59:42,252 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:42,255 EPOCH 1 done: loss 0.7451 - lr 0.0000\n",
      "2019-10-19 14:00:59,918 DEV : loss 1.0670547485351562 - score 0.6092\n",
      "2019-10-19 14:01:02,631 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:01:10,031 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:01:10,034 Testing using best model ...\n",
      "2019-10-19 14:01:10,035 loading file absolute_sampling/6/best-model.pt\n",
      "2019-10-19 14:02:22,188 0.626\t0.6261\t0.626\n",
      "2019-10-19 14:02:22,190 \n",
      "MICRO_AVG: acc 0.4557 - f1-score 0.626\n",
      "MACRO_AVG: acc 0.4269 - f1-score 0.587\n",
      "0          tp: 4669 - fp: 3418 - fn: 321 - tn: 1593 - precision: 0.5773 - recall: 0.9357 - accuracy: 0.5553 - f1-score: 0.7141\n",
      "1          tp: 1592 - fp: 322 - fn: 3418 - tn: 4669 - precision: 0.8318 - recall: 0.3178 - accuracy: 0.2986 - f1-score: 0.4599\n",
      "2019-10-19 14:02:22,190 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:03:35,198 Reading data from absolute_sampling\n",
      "2019-10-19 14:03:35,199 Train: absolute_sampling/labelled_128_1571493815.csv\n",
      "2019-10-19 14:03:35,201 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:03:35,201 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:03:35,280 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:35,281 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:03:35,282 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:35,283 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:03:35,284 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:35,284 Parameters:\n",
      "2019-10-19 14:03:35,285  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:03:35,287  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:03:35,288  - patience: \"5\"\n",
      "2019-10-19 14:03:35,289  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:03:35,289  - max_epochs: \"1\"\n",
      "2019-10-19 14:03:35,290  - shuffle: \"True\"\n",
      "2019-10-19 14:03:35,291  - train_with_dev: \"False\"\n",
      "2019-10-19 14:03:35,291 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:35,292 Model training base path: \"absolute_sampling/7\"\n",
      "2019-10-19 14:03:35,293 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:35,297 Device: cuda:2\n",
      "2019-10-19 14:03:35,298 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:35,299 Embeddings storage mode: gpu\n",
      "2019-10-19 14:03:35,303 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:36,399 epoch 1 - iter 0/4 - loss 0.55008072 - samples/sec: 137.01\n",
      "2019-10-19 14:03:36,675 epoch 1 - iter 1/4 - loss 0.59722903 - samples/sec: 132.19\n",
      "2019-10-19 14:03:36,910 epoch 1 - iter 2/4 - loss 0.58091819 - samples/sec: 156.89\n",
      "2019-10-19 14:03:37,149 epoch 1 - iter 3/4 - loss 0.59848329 - samples/sec: 150.55\n",
      "2019-10-19 14:03:37,481 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:37,483 EPOCH 1 done: loss 0.5985 - lr 0.0000\n",
      "2019-10-19 14:04:47,715 DEV : loss 1.1675876379013062 - score 0.5947\n",
      "2019-10-19 14:04:50,352 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:04:58,933 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:04:58,934 Testing using best model ...\n",
      "2019-10-19 14:04:58,935 loading file absolute_sampling/7/best-model.pt\n",
      "2019-10-19 14:06:18,692 0.6088\t0.6089\t0.6088\n",
      "2019-10-19 14:06:18,694 \n",
      "MICRO_AVG: acc 0.4377 - f1-score 0.6088\n",
      "MACRO_AVG: acc 0.4022 - f1-score 0.5581499999999999\n",
      "0          tp: 4739 - fp: 3660 - fn: 251 - tn: 1351 - precision: 0.5642 - recall: 0.9497 - accuracy: 0.5479 - f1-score: 0.7079\n",
      "1          tp: 1350 - fp: 252 - fn: 3660 - tn: 4739 - precision: 0.8427 - recall: 0.2695 - accuracy: 0.2566 - f1-score: 0.4084\n",
      "2019-10-19 14:06:18,695 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:07:23,879 Reading data from absolute_sampling\n",
      "2019-10-19 14:07:23,881 Train: absolute_sampling/labelled_128_1571494043.csv\n",
      "2019-10-19 14:07:23,882 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:07:23,883 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:07:23,962 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:23,964 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:07:23,965 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:23,965 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:07:23,967 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:23,969 Parameters:\n",
      "2019-10-19 14:07:23,970  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:07:23,973  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:07:23,974  - patience: \"5\"\n",
      "2019-10-19 14:07:23,976  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:07:23,977  - max_epochs: \"1\"\n",
      "2019-10-19 14:07:23,978  - shuffle: \"True\"\n",
      "2019-10-19 14:07:23,980  - train_with_dev: \"False\"\n",
      "2019-10-19 14:07:23,981 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:23,982 Model training base path: \"absolute_sampling/8\"\n",
      "2019-10-19 14:07:23,982 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:23,983 Device: cuda:2\n",
      "2019-10-19 14:07:23,984 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:23,984 Embeddings storage mode: gpu\n",
      "2019-10-19 14:07:23,989 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:24,973 epoch 1 - iter 0/4 - loss 0.62525296 - samples/sec: 143.24\n",
      "2019-10-19 14:07:25,242 epoch 1 - iter 1/4 - loss 0.59924206 - samples/sec: 131.24\n",
      "2019-10-19 14:07:25,462 epoch 1 - iter 2/4 - loss 0.55864683 - samples/sec: 155.09\n",
      "2019-10-19 14:07:25,694 epoch 1 - iter 3/4 - loss 0.56623819 - samples/sec: 148.46\n",
      "2019-10-19 14:07:26,016 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:26,018 EPOCH 1 done: loss 0.5662 - lr 0.0000\n",
      "2019-10-19 14:08:43,756 DEV : loss 1.2771576642990112 - score 0.5807\n",
      "2019-10-19 14:08:46,606 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:08:54,833 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:08:54,834 Testing using best model ...\n",
      "2019-10-19 14:08:54,835 loading file absolute_sampling/8/best-model.pt\n",
      "2019-10-19 14:10:06,623 0.5943\t0.5944\t0.5943\n",
      "2019-10-19 14:10:06,625 \n",
      "MICRO_AVG: acc 0.4228 - f1-score 0.5943\n",
      "MACRO_AVG: acc 0.3814 - f1-score 0.5324\n",
      "0          tp: 4792 - fp: 3858 - fn: 198 - tn: 1153 - precision: 0.5540 - recall: 0.9603 - accuracy: 0.5416 - f1-score: 0.7026\n",
      "1          tp: 1152 - fp: 199 - fn: 3858 - tn: 4792 - precision: 0.8527 - recall: 0.2299 - accuracy: 0.2212 - f1-score: 0.3622\n",
      "2019-10-19 14:10:06,626 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:11:10,626 Reading data from absolute_sampling\n",
      "2019-10-19 14:11:10,627 Train: absolute_sampling/labelled_128_1571494270.csv\n",
      "2019-10-19 14:11:10,637 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:11:10,638 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:11:10,695 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:10,697 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:11:10,698 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:10,699 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:11:10,701 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:10,702 Parameters:\n",
      "2019-10-19 14:11:10,704  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:11:10,705  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:11:10,709  - patience: \"5\"\n",
      "2019-10-19 14:11:10,710  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:11:10,711  - max_epochs: \"1\"\n",
      "2019-10-19 14:11:10,711  - shuffle: \"True\"\n",
      "2019-10-19 14:11:10,713  - train_with_dev: \"False\"\n",
      "2019-10-19 14:11:10,714 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:10,714 Model training base path: \"absolute_sampling/9\"\n",
      "2019-10-19 14:11:10,715 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:10,716 Device: cuda:2\n",
      "2019-10-19 14:11:10,717 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:10,717 Embeddings storage mode: gpu\n",
      "2019-10-19 14:11:10,722 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:11,476 epoch 1 - iter 0/4 - loss 0.50155264 - samples/sec: 137.28\n",
      "2019-10-19 14:11:11,760 epoch 1 - iter 1/4 - loss 0.59302568 - samples/sec: 126.93\n",
      "2019-10-19 14:11:12,030 epoch 1 - iter 2/4 - loss 0.58256066 - samples/sec: 129.69\n",
      "2019-10-19 14:11:12,287 epoch 1 - iter 3/4 - loss 0.56364419 - samples/sec: 137.44\n",
      "2019-10-19 14:11:12,552 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:12,554 EPOCH 1 done: loss 0.5636 - lr 0.0000\n",
      "2019-10-19 14:17:51,341 DEV : loss 1.3958760499954224 - score 0.5678\n",
      "2019-10-19 14:17:53,626 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:18:00,146 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:18:00,148 Testing using best model ...\n",
      "2019-10-19 14:18:00,149 loading file absolute_sampling/9/best-model.pt\n",
      "2019-10-19 14:19:18,044 0.5789\t0.579\t0.5789\n",
      "2019-10-19 14:19:18,046 \n",
      "MICRO_AVG: acc 0.4074 - f1-score 0.5789\n",
      "MACRO_AVG: acc 0.3598 - f1-score 0.5044\n",
      "0          tp: 4834 - fp: 4054 - fn: 156 - tn: 957 - precision: 0.5439 - recall: 0.9687 - accuracy: 0.5345 - f1-score: 0.6966\n",
      "1          tp: 956 - fp: 157 - fn: 4054 - tn: 4834 - precision: 0.8589 - recall: 0.1908 - accuracy: 0.1850 - f1-score: 0.3122\n",
      "2019-10-19 14:19:18,046 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:20:17,749 Reading data from absolute_sampling\n",
      "2019-10-19 14:20:17,750 Train: absolute_sampling/labelled_128_1571494817.csv\n",
      "2019-10-19 14:20:17,751 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:20:17,752 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:20:17,862 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:17,863 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:20:17,864 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:17,865 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:20:17,866 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:17,866 Parameters:\n",
      "2019-10-19 14:20:17,867  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:20:17,868  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:20:17,870  - patience: \"5\"\n",
      "2019-10-19 14:20:17,870  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:20:17,871  - max_epochs: \"1\"\n",
      "2019-10-19 14:20:17,872  - shuffle: \"True\"\n",
      "2019-10-19 14:20:17,872  - train_with_dev: \"False\"\n",
      "2019-10-19 14:20:17,874 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:17,875 Model training base path: \"absolute_sampling/10\"\n",
      "2019-10-19 14:20:17,875 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:17,876 Device: cuda:2\n",
      "2019-10-19 14:20:17,876 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:17,877 Embeddings storage mode: gpu\n",
      "2019-10-19 14:20:17,880 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:18,876 epoch 1 - iter 0/4 - loss 0.68260503 - samples/sec: 133.69\n",
      "2019-10-19 14:20:19,158 epoch 1 - iter 1/4 - loss 0.68646491 - samples/sec: 132.11\n",
      "2019-10-19 14:20:19,376 epoch 1 - iter 2/4 - loss 0.68467267 - samples/sec: 159.36\n",
      "2019-10-19 14:20:19,594 epoch 1 - iter 3/4 - loss 0.66079609 - samples/sec: 159.12\n",
      "2019-10-19 14:20:19,843 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:19,844 EPOCH 1 done: loss 0.6608 - lr 0.0000\n",
      "2019-10-19 14:21:34,943 DEV : loss 1.5221107006072998 - score 0.5551\n",
      "2019-10-19 14:21:37,216 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:21:43,702 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:21:43,704 Testing using best model ...\n",
      "2019-10-19 14:21:43,705 loading file absolute_sampling/10/best-model.pt\n",
      "2019-10-19 14:22:53,828 0.566\t0.5661\t0.566\n",
      "2019-10-19 14:22:53,830 \n",
      "MICRO_AVG: acc 0.3948 - f1-score 0.566\n",
      "MACRO_AVG: acc 0.3415 - f1-score 0.4794\n",
      "0          tp: 4871 - fp: 4220 - fn: 119 - tn: 791 - precision: 0.5358 - recall: 0.9762 - accuracy: 0.5289 - f1-score: 0.6919\n",
      "1          tp: 790 - fp: 120 - fn: 4220 - tn: 4871 - precision: 0.8681 - recall: 0.1577 - accuracy: 0.1540 - f1-score: 0.2669\n",
      "2019-10-19 14:22:53,831 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:23:52,978 Reading data from absolute_sampling\n",
      "2019-10-19 14:23:52,979 Train: absolute_sampling/labelled_128_1571495032.csv\n",
      "2019-10-19 14:23:52,981 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:23:52,982 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:23:53,028 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:53,029 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:23:53,030 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:53,031 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:23:53,031 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:53,032 Parameters:\n",
      "2019-10-19 14:23:53,033  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:23:53,033  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:23:53,034  - patience: \"5\"\n",
      "2019-10-19 14:23:53,035  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:23:53,035  - max_epochs: \"1\"\n",
      "2019-10-19 14:23:53,036  - shuffle: \"True\"\n",
      "2019-10-19 14:23:53,037  - train_with_dev: \"False\"\n",
      "2019-10-19 14:23:53,037 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:53,038 Model training base path: \"absolute_sampling/11\"\n",
      "2019-10-19 14:23:53,039 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:53,039 Device: cuda:2\n",
      "2019-10-19 14:23:53,040 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:53,041 Embeddings storage mode: gpu\n",
      "2019-10-19 14:23:53,046 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:53,838 epoch 1 - iter 0/4 - loss 0.57287568 - samples/sec: 133.90\n",
      "2019-10-19 14:23:54,103 epoch 1 - iter 1/4 - loss 0.55215690 - samples/sec: 141.01\n",
      "2019-10-19 14:23:54,305 epoch 1 - iter 2/4 - loss 0.53626357 - samples/sec: 169.96\n",
      "2019-10-19 14:23:54,519 epoch 1 - iter 3/4 - loss 0.61056092 - samples/sec: 163.76\n",
      "2019-10-19 14:23:54,800 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:54,802 EPOCH 1 done: loss 0.6106 - lr 0.0000\n",
      "2019-10-19 14:25:08,740 DEV : loss 1.6554350852966309 - score 0.5456\n",
      "2019-10-19 14:25:10,948 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:25:18,125 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:25:18,126 Testing using best model ...\n",
      "2019-10-19 14:25:18,128 loading file absolute_sampling/11/best-model.pt\n",
      "2019-10-19 14:26:27,321 0.555\t0.5551\t0.555\n",
      "2019-10-19 14:26:27,323 \n",
      "MICRO_AVG: acc 0.3842 - f1-score 0.555\n",
      "MACRO_AVG: acc 0.326 - f1-score 0.45725\n",
      "0          tp: 4898 - fp: 4357 - fn: 92 - tn: 654 - precision: 0.5292 - recall: 0.9816 - accuracy: 0.5240 - f1-score: 0.6877\n",
      "1          tp: 653 - fp: 93 - fn: 4357 - tn: 4898 - precision: 0.8753 - recall: 0.1303 - accuracy: 0.1280 - f1-score: 0.2268\n",
      "2019-10-19 14:26:27,324 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:27:32,126 Reading data from absolute_sampling\n",
      "2019-10-19 14:27:32,127 Train: absolute_sampling/labelled_128_1571495252.csv\n",
      "2019-10-19 14:27:32,128 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:27:32,129 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:27:32,170 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:32,171 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:27:32,172 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:32,173 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:27:32,174 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:32,174 Parameters:\n",
      "2019-10-19 14:27:32,175  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:27:32,175  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:27:32,176  - patience: \"5\"\n",
      "2019-10-19 14:27:32,176  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:27:32,177  - max_epochs: \"1\"\n",
      "2019-10-19 14:27:32,178  - shuffle: \"True\"\n",
      "2019-10-19 14:27:32,179  - train_with_dev: \"False\"\n",
      "2019-10-19 14:27:32,180 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:32,181 Model training base path: \"absolute_sampling/12\"\n",
      "2019-10-19 14:27:32,182 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:32,182 Device: cuda:2\n",
      "2019-10-19 14:27:32,183 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:32,183 Embeddings storage mode: gpu\n",
      "2019-10-19 14:27:32,186 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:32,884 epoch 1 - iter 0/4 - loss 0.50818437 - samples/sec: 135.14\n",
      "2019-10-19 14:27:33,112 epoch 1 - iter 1/4 - loss 0.47331169 - samples/sec: 162.41\n",
      "2019-10-19 14:27:33,328 epoch 1 - iter 2/4 - loss 0.47063221 - samples/sec: 162.49\n",
      "2019-10-19 14:27:33,548 epoch 1 - iter 3/4 - loss 0.47282027 - samples/sec: 156.27\n",
      "2019-10-19 14:27:33,779 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:33,780 EPOCH 1 done: loss 0.4728 - lr 0.0000\n",
      "2019-10-19 14:28:41,801 DEV : loss 1.793656349182129 - score 0.537\n",
      "2019-10-19 14:28:43,927 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:28:50,454 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:28:50,455 Testing using best model ...\n",
      "2019-10-19 14:28:50,456 loading file absolute_sampling/12/best-model.pt\n",
      "2019-10-19 14:30:06,163 0.5447\t0.5448\t0.5447\n",
      "2019-10-19 14:30:06,164 \n",
      "MICRO_AVG: acc 0.3744 - f1-score 0.5447\n",
      "MACRO_AVG: acc 0.3116 - f1-score 0.436\n",
      "0          tp: 4920 - fp: 4482 - fn: 70 - tn: 529 - precision: 0.5233 - recall: 0.9860 - accuracy: 0.5194 - f1-score: 0.6837\n",
      "1          tp: 528 - fp: 71 - fn: 4482 - tn: 4920 - precision: 0.8815 - recall: 0.1054 - accuracy: 0.1039 - f1-score: 0.1883\n",
      "2019-10-19 14:30:06,165 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:31:06,202 Reading data from absolute_sampling\n",
      "2019-10-19 14:31:06,203 Train: absolute_sampling/labelled_128_1571495466.csv\n",
      "2019-10-19 14:31:06,204 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:31:06,204 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:31:06,257 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:06,258 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:31:06,260 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:06,260 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:31:06,261 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:06,264 Parameters:\n",
      "2019-10-19 14:31:06,264  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:31:06,265  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:31:06,266  - patience: \"5\"\n",
      "2019-10-19 14:31:06,266  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:31:06,267  - max_epochs: \"1\"\n",
      "2019-10-19 14:31:06,268  - shuffle: \"True\"\n",
      "2019-10-19 14:31:06,268  - train_with_dev: \"False\"\n",
      "2019-10-19 14:31:06,269 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:06,270 Model training base path: \"absolute_sampling/13\"\n",
      "2019-10-19 14:31:06,270 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:06,271 Device: cuda:2\n",
      "2019-10-19 14:31:06,272 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:06,272 Embeddings storage mode: gpu\n",
      "2019-10-19 14:31:06,277 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:07,011 epoch 1 - iter 0/4 - loss 0.72314638 - samples/sec: 142.28\n",
      "2019-10-19 14:31:07,228 epoch 1 - iter 1/4 - loss 0.66678372 - samples/sec: 162.79\n",
      "2019-10-19 14:31:07,417 epoch 1 - iter 2/4 - loss 0.61030574 - samples/sec: 182.40\n",
      "2019-10-19 14:31:07,654 epoch 1 - iter 3/4 - loss 0.59309170 - samples/sec: 151.93\n",
      "2019-10-19 14:31:07,902 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:07,903 EPOCH 1 done: loss 0.5931 - lr 0.0000\n",
      "2019-10-19 14:32:22,770 DEV : loss 1.937107801437378 - score 0.5306\n",
      "2019-10-19 14:32:24,978 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:32:31,702 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:32:31,703 Testing using best model ...\n",
      "2019-10-19 14:32:31,705 loading file absolute_sampling/13/best-model.pt\n",
      "2019-10-19 14:33:41,576 0.5353\t0.5354\t0.5353\n",
      "2019-10-19 14:33:41,577 \n",
      "MICRO_AVG: acc 0.3655 - f1-score 0.5353\n",
      "MACRO_AVG: acc 0.299 - f1-score 0.41659999999999997\n",
      "0          tp: 4933 - fp: 4589 - fn: 57 - tn: 422 - precision: 0.5181 - recall: 0.9886 - accuracy: 0.5150 - f1-score: 0.6799\n",
      "1          tp: 421 - fp: 58 - fn: 4589 - tn: 4933 - precision: 0.8789 - recall: 0.0840 - accuracy: 0.0831 - f1-score: 0.1533\n",
      "2019-10-19 14:33:41,578 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:34:41,105 Reading data from absolute_sampling\n",
      "2019-10-19 14:34:41,106 Train: absolute_sampling/labelled_128_1571495681.csv\n",
      "2019-10-19 14:34:41,107 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:34:41,108 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:34:41,153 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:41,155 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:34:41,155 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:41,156 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:34:41,157 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:41,158 Parameters:\n",
      "2019-10-19 14:34:41,159  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:34:41,160  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:34:41,160  - patience: \"5\"\n",
      "2019-10-19 14:34:41,161  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:34:41,162  - max_epochs: \"1\"\n",
      "2019-10-19 14:34:41,162  - shuffle: \"True\"\n",
      "2019-10-19 14:34:41,163  - train_with_dev: \"False\"\n",
      "2019-10-19 14:34:41,163 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:41,165 Model training base path: \"absolute_sampling/14\"\n",
      "2019-10-19 14:34:41,166 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:41,166 Device: cuda:2\n",
      "2019-10-19 14:34:41,167 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:41,168 Embeddings storage mode: gpu\n",
      "2019-10-19 14:34:41,171 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:41,855 epoch 1 - iter 0/4 - loss 0.63024974 - samples/sec: 237.99\n",
      "2019-10-19 14:34:42,077 epoch 1 - iter 1/4 - loss 0.58361489 - samples/sec: 156.58\n",
      "2019-10-19 14:34:42,326 epoch 1 - iter 2/4 - loss 0.55005346 - samples/sec: 136.52\n",
      "2019-10-19 14:34:42,562 epoch 1 - iter 3/4 - loss 0.56703804 - samples/sec: 151.93\n",
      "2019-10-19 14:34:42,804 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:42,806 EPOCH 1 done: loss 0.5670 - lr 0.0000\n",
      "2019-10-19 14:35:56,867 DEV : loss 2.0822882652282715 - score 0.5225\n",
      "2019-10-19 14:35:59,045 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:36:05,674 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:36:05,675 Testing using best model ...\n",
      "2019-10-19 14:36:05,676 loading file absolute_sampling/14/best-model.pt\n",
      "2019-10-19 14:37:22,752 0.5276\t0.5277\t0.5276\n",
      "2019-10-19 14:37:22,754 \n",
      "MICRO_AVG: acc 0.3584 - f1-score 0.5276\n",
      "MACRO_AVG: acc 0.2881 - f1-score 0.39890000000000003\n",
      "0          tp: 4953 - fp: 4686 - fn: 37 - tn: 325 - precision: 0.5138 - recall: 0.9926 - accuracy: 0.5119 - f1-score: 0.6771\n",
      "1          tp: 324 - fp: 38 - fn: 4686 - tn: 4953 - precision: 0.8950 - recall: 0.0647 - accuracy: 0.0642 - f1-score: 0.1207\n",
      "2019-10-19 14:37:22,755 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:38:22,361 Reading data from absolute_sampling\n",
      "2019-10-19 14:38:22,362 Train: absolute_sampling/labelled_128_1571495902.csv\n",
      "2019-10-19 14:38:22,363 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:38:22,364 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:38:22,409 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:22,410 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:38:22,411 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:22,411 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:38:22,412 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:22,413 Parameters:\n",
      "2019-10-19 14:38:22,413  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:38:22,414  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:38:22,414  - patience: \"5\"\n",
      "2019-10-19 14:38:22,415  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:38:22,415  - max_epochs: \"1\"\n",
      "2019-10-19 14:38:22,416  - shuffle: \"True\"\n",
      "2019-10-19 14:38:22,416  - train_with_dev: \"False\"\n",
      "2019-10-19 14:38:22,417 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:22,418 Model training base path: \"absolute_sampling/15\"\n",
      "2019-10-19 14:38:22,418 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:22,419 Device: cuda:2\n",
      "2019-10-19 14:38:22,419 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:22,420 Embeddings storage mode: gpu\n",
      "2019-10-19 14:38:22,425 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:23,113 epoch 1 - iter 0/4 - loss 0.68143284 - samples/sec: 166.07\n",
      "2019-10-19 14:38:23,320 epoch 1 - iter 1/4 - loss 0.60886213 - samples/sec: 170.09\n",
      "2019-10-19 14:38:23,490 epoch 1 - iter 2/4 - loss 0.61403773 - samples/sec: 222.49\n",
      "2019-10-19 14:38:23,673 epoch 1 - iter 3/4 - loss 0.60423192 - samples/sec: 207.74\n",
      "2019-10-19 14:38:23,928 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:23,930 EPOCH 1 done: loss 0.6042 - lr 0.0000\n",
      "2019-10-19 14:39:31,926 DEV : loss 2.2269017696380615 - score 0.5176\n",
      "2019-10-19 14:39:34,134 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:39:40,649 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:39:40,650 Testing using best model ...\n",
      "2019-10-19 14:39:40,652 loading file absolute_sampling/15/best-model.pt\n",
      "2019-10-19 14:40:57,195 0.5223\t0.5224\t0.5223\n",
      "2019-10-19 14:40:57,196 \n",
      "MICRO_AVG: acc 0.3535 - f1-score 0.5223\n",
      "MACRO_AVG: acc 0.2808 - f1-score 0.387\n",
      "0          tp: 4962 - fp: 4748 - fn: 28 - tn: 263 - precision: 0.5110 - recall: 0.9944 - accuracy: 0.5096 - f1-score: 0.6751\n",
      "1          tp: 262 - fp: 29 - fn: 4748 - tn: 4962 - precision: 0.9003 - recall: 0.0523 - accuracy: 0.0520 - f1-score: 0.0989\n",
      "2019-10-19 14:40:57,197 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:41:57,283 Reading data from absolute_sampling\n",
      "2019-10-19 14:41:57,284 Train: absolute_sampling/labelled_128_1571496117.csv\n",
      "2019-10-19 14:41:57,285 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:41:57,285 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:41:57,328 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:57,329 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:41:57,330 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:57,330 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:41:57,331 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:57,332 Parameters:\n",
      "2019-10-19 14:41:57,332  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:41:57,333  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:41:57,333  - patience: \"5\"\n",
      "2019-10-19 14:41:57,334  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:41:57,334  - max_epochs: \"1\"\n",
      "2019-10-19 14:41:57,335  - shuffle: \"True\"\n",
      "2019-10-19 14:41:57,336  - train_with_dev: \"False\"\n",
      "2019-10-19 14:41:57,336 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:57,337 Model training base path: \"absolute_sampling/16\"\n",
      "2019-10-19 14:41:57,338 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:57,341 Device: cuda:2\n",
      "2019-10-19 14:41:57,342 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:57,343 Embeddings storage mode: gpu\n",
      "2019-10-19 14:41:57,346 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:58,082 epoch 1 - iter 0/4 - loss 0.75779432 - samples/sec: 148.32\n",
      "2019-10-19 14:41:58,305 epoch 1 - iter 1/4 - loss 0.61971948 - samples/sec: 155.23\n",
      "2019-10-19 14:41:58,532 epoch 1 - iter 2/4 - loss 0.59424086 - samples/sec: 150.89\n",
      "2019-10-19 14:41:58,747 epoch 1 - iter 3/4 - loss 0.55616544 - samples/sec: 159.36\n",
      "2019-10-19 14:41:59,008 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:59,010 EPOCH 1 done: loss 0.5562 - lr 0.0000\n",
      "2019-10-19 14:43:13,199 DEV : loss 2.371542453765869 - score 0.514\n",
      "2019-10-19 14:43:15,530 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:43:22,065 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:43:22,066 Testing using best model ...\n",
      "2019-10-19 14:43:22,067 loading file absolute_sampling/16/best-model.pt\n",
      "2019-10-19 14:44:31,449 0.5173\t0.5174\t0.5173\n",
      "2019-10-19 14:44:31,451 \n",
      "MICRO_AVG: acc 0.349 - f1-score 0.5173\n",
      "MACRO_AVG: acc 0.2742 - f1-score 0.376\n",
      "0          tp: 4967 - fp: 4803 - fn: 23 - tn: 208 - precision: 0.5084 - recall: 0.9954 - accuracy: 0.5072 - f1-score: 0.6730\n",
      "1          tp: 207 - fp: 24 - fn: 4803 - tn: 4967 - precision: 0.8961 - recall: 0.0413 - accuracy: 0.0411 - f1-score: 0.0790\n",
      "2019-10-19 14:44:31,452 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:45:35,999 Reading data from absolute_sampling\n",
      "2019-10-19 14:45:36,000 Train: absolute_sampling/labelled_128_1571496335.csv\n",
      "2019-10-19 14:45:36,001 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:45:36,002 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:45:36,051 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:36,052 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:45:36,053 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:36,054 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:45:36,054 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:36,055 Parameters:\n",
      "2019-10-19 14:45:36,056  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:45:36,056  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:45:36,057  - patience: \"5\"\n",
      "2019-10-19 14:45:36,058  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:45:36,058  - max_epochs: \"1\"\n",
      "2019-10-19 14:45:36,059  - shuffle: \"True\"\n",
      "2019-10-19 14:45:36,059  - train_with_dev: \"False\"\n",
      "2019-10-19 14:45:36,060 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:36,061 Model training base path: \"absolute_sampling/17\"\n",
      "2019-10-19 14:45:36,061 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:36,062 Device: cuda:2\n",
      "2019-10-19 14:45:36,062 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:36,063 Embeddings storage mode: gpu\n",
      "2019-10-19 14:45:36,065 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:36,769 epoch 1 - iter 0/4 - loss 0.31891492 - samples/sec: 187.66\n",
      "2019-10-19 14:45:36,979 epoch 1 - iter 1/4 - loss 0.41292472 - samples/sec: 168.00\n",
      "2019-10-19 14:45:37,192 epoch 1 - iter 2/4 - loss 0.45443117 - samples/sec: 161.74\n",
      "2019-10-19 14:45:37,401 epoch 1 - iter 3/4 - loss 0.42328072 - samples/sec: 164.96\n",
      "2019-10-19 14:45:37,673 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:37,675 EPOCH 1 done: loss 0.4233 - lr 0.0000\n",
      "2019-10-19 14:46:45,767 DEV : loss 2.5197598934173584 - score 0.511\n",
      "2019-10-19 14:46:48,006 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:46:54,568 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:46:54,569 Testing using best model ...\n",
      "2019-10-19 14:46:54,570 loading file absolute_sampling/17/best-model.pt\n",
      "2019-10-19 14:48:10,992 0.5145\t0.5146\t0.5145\n",
      "2019-10-19 14:48:10,994 \n",
      "MICRO_AVG: acc 0.3464 - f1-score 0.5145\n",
      "MACRO_AVG: acc 0.2704 - f1-score 0.36955000000000005\n",
      "0          tp: 4971 - fp: 4835 - fn: 19 - tn: 176 - precision: 0.5069 - recall: 0.9962 - accuracy: 0.5060 - f1-score: 0.6719\n",
      "1          tp: 175 - fp: 20 - fn: 4835 - tn: 4971 - precision: 0.8974 - recall: 0.0349 - accuracy: 0.0348 - f1-score: 0.0672\n",
      "2019-10-19 14:48:10,995 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:49:10,530 Reading data from absolute_sampling\n",
      "2019-10-19 14:49:10,531 Train: absolute_sampling/labelled_128_1571496550.csv\n",
      "2019-10-19 14:49:10,533 Dev: absolute_sampling/valid.csv\n",
      "2019-10-19 14:49:10,534 Test: absolute_sampling/test.csv\n",
      "2019-10-19 14:49:10,595 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:10,596 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:49:10,596 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:10,597 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:49:10,598 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:10,598 Parameters:\n",
      "2019-10-19 14:49:10,599  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:49:10,599  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:49:10,600  - patience: \"5\"\n",
      "2019-10-19 14:49:10,602  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:49:10,602  - max_epochs: \"1\"\n",
      "2019-10-19 14:49:10,603  - shuffle: \"True\"\n",
      "2019-10-19 14:49:10,604  - train_with_dev: \"False\"\n",
      "2019-10-19 14:49:10,604 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:10,605 Model training base path: \"absolute_sampling/18\"\n",
      "2019-10-19 14:49:10,606 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:10,606 Device: cuda:2\n",
      "2019-10-19 14:49:10,607 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:10,607 Embeddings storage mode: gpu\n",
      "2019-10-19 14:49:10,610 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:11,430 epoch 1 - iter 0/4 - loss 0.50743657 - samples/sec: 142.92\n",
      "2019-10-19 14:49:11,559 epoch 1 - iter 1/4 - loss 0.55285195 - samples/sec: 288.70\n",
      "2019-10-19 14:49:11,772 epoch 1 - iter 2/4 - loss 0.53117602 - samples/sec: 162.16\n",
      "2019-10-19 14:49:11,970 epoch 1 - iter 3/4 - loss 0.48790777 - samples/sec: 178.74\n",
      "2019-10-19 14:49:12,247 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:12,250 EPOCH 1 done: loss 0.4879 - lr 0.0000\n"
     ]
    }
   ],
   "source": [
    "for step_num in range(1, 100):\n",
    "    classifier, opt_state = learner.step(classifier,\n",
    "                                         step_num=step_num,\n",
    "                                         optimizer_state=opt_state,\n",
    "                                         max_sample_size=100000, \n",
    "                                         labelling_step_size=128,\n",
    "                                         step_lr=1e-5, \n",
    "                                         sampling_method='absolute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
