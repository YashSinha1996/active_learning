{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 31\n",
    "seed_everything(SEED)\n",
    "\n",
    "import flair\n",
    "flair.device = torch.device('cuda:0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oracle import HybridOracle\n",
    "from active_learner import ActiveLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'weighted_sampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../twitter_dataset/train_10.csv\n",
      "Starting with labelled data\n"
     ]
    }
   ],
   "source": [
    "oracle = HybridOracle(\n",
    "    experiment_name=EXPERIMENT,\n",
    "    all_data_file='../twitter_dataset/train_10.csv',\n",
    "    valid_file='../twitter_dataset/valid.csv',\n",
    "    test_file='../twitter_dataset/test.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = oracle.get_all_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ActiveLearner(sentences,\n",
    "                        experiment_name=EXPERIMENT,\n",
    "                        oracle=oracle, \n",
    "                        embeddings_storage_mode='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Dictionary, Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "                    FlairEmbeddings('news-forward'),\n",
    "                    FlairEmbeddings('news-backward'),\n",
    "                   ]\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )\n",
    "\n",
    "label_dict = Dictionary()\n",
    "label_dict.idx2item = [b'0', b'1']\n",
    "label_dict.item2idx = {b'0': 0, b'1': 1}\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:35:30,052 loading file ../twitter_dataset/0_1/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.load('../twitter_dataset/0_1/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:36:37,777 Reading data from weighted_sampling\n",
      "2019-10-19 13:36:37,779 Train: weighted_sampling/labelled_128_1571492197.csv\n",
      "2019-10-19 13:36:37,785 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 13:36:37,786 Test: weighted_sampling/test.csv\n",
      "2019-10-19 13:36:37,864 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:37,866 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:36:37,867 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:37,867 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:36:37,868 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:37,868 Parameters:\n",
      "2019-10-19 13:36:37,869  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:36:37,870  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:36:37,871  - patience: \"5\"\n",
      "2019-10-19 13:36:37,871  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:36:37,872  - max_epochs: \"1\"\n",
      "2019-10-19 13:36:37,872  - shuffle: \"True\"\n",
      "2019-10-19 13:36:37,873  - train_with_dev: \"False\"\n",
      "2019-10-19 13:36:37,875 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:37,876 Model training base path: \"weighted_sampling/0\"\n",
      "2019-10-19 13:36:37,876 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:37,877 Device: cuda:0\n",
      "2019-10-19 13:36:37,877 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:37,880 Embeddings storage mode: gpu\n",
      "2019-10-19 13:36:37,882 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:38,491 epoch 1 - iter 0/4 - loss 1.07179964 - samples/sec: 133.92\n",
      "2019-10-19 13:36:38,752 epoch 1 - iter 1/4 - loss 0.86906010 - samples/sec: 137.97\n",
      "2019-10-19 13:36:39,010 epoch 1 - iter 2/4 - loss 0.85287694 - samples/sec: 139.96\n",
      "2019-10-19 13:36:39,247 epoch 1 - iter 3/4 - loss 0.90740299 - samples/sec: 144.85\n",
      "2019-10-19 13:36:39,397 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:36:39,398 EPOCH 1 done: loss 0.9074 - lr 0.0000\n",
      "2019-10-19 13:37:48,757 DEV : loss 0.7222411632537842 - score 0.6896\n",
      "2019-10-19 13:37:51,891 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:38:02,032 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:38:02,034 Testing using best model ...\n",
      "2019-10-19 13:38:02,036 loading file weighted_sampling/0/best-model.pt\n",
      "2019-10-19 13:39:20,795 0.7015\t0.7016\t0.7015\n",
      "2019-10-19 13:39:20,798 \n",
      "MICRO_AVG: acc 0.5403 - f1-score 0.7015\n",
      "MACRO_AVG: acc 0.54 - f1-score 0.70125\n",
      "0          tp: 3675 - fp: 1669 - fn: 1315 - tn: 3342 - precision: 0.6877 - recall: 0.7365 - accuracy: 0.5519 - f1-score: 0.7113\n",
      "1          tp: 3341 - fp: 1316 - fn: 1669 - tn: 3675 - precision: 0.7174 - recall: 0.6669 - accuracy: 0.5281 - f1-score: 0.6912\n",
      "2019-10-19 13:39:20,800 ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "classifier, opt_state = learner.step(classifier, step_num=0, max_sample_size=100000, labelling_step_size=128, step_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:40:24,696 Reading data from weighted_sampling\n",
      "2019-10-19 13:40:24,697 Train: weighted_sampling/labelled_128_1571492424.csv\n",
      "2019-10-19 13:40:24,698 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 13:40:24,700 Test: weighted_sampling/test.csv\n",
      "2019-10-19 13:40:24,745 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:24,746 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:40:24,747 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:24,751 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:40:24,751 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:24,752 Parameters:\n",
      "2019-10-19 13:40:24,753  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:40:24,753  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:40:24,754  - patience: \"5\"\n",
      "2019-10-19 13:40:24,754  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:40:24,755  - max_epochs: \"1\"\n",
      "2019-10-19 13:40:24,756  - shuffle: \"True\"\n",
      "2019-10-19 13:40:24,756  - train_with_dev: \"False\"\n",
      "2019-10-19 13:40:24,757 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:24,758 Model training base path: \"weighted_sampling/1\"\n",
      "2019-10-19 13:40:24,758 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:24,759 Device: cuda:0\n",
      "2019-10-19 13:40:24,760 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:24,760 Embeddings storage mode: gpu\n",
      "2019-10-19 13:40:24,766 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:25,532 epoch 1 - iter 0/4 - loss 0.88997674 - samples/sec: 128.11\n",
      "2019-10-19 13:40:25,788 epoch 1 - iter 1/4 - loss 0.92397848 - samples/sec: 134.55\n",
      "2019-10-19 13:40:26,033 epoch 1 - iter 2/4 - loss 0.97470194 - samples/sec: 147.73\n",
      "2019-10-19 13:40:26,278 epoch 1 - iter 3/4 - loss 0.91934694 - samples/sec: 139.08\n",
      "2019-10-19 13:40:26,469 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:40:26,471 EPOCH 1 done: loss 0.9193 - lr 0.0000\n",
      "2019-10-19 13:41:43,962 DEV : loss 0.7426920533180237 - score 0.685\n",
      "2019-10-19 13:41:47,796 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:41:57,104 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:41:57,105 Testing using best model ...\n",
      "2019-10-19 13:41:57,107 loading file weighted_sampling/1/best-model.pt\n",
      "2019-10-19 13:43:08,716 0.6921\t0.6922\t0.6921\n",
      "2019-10-19 13:43:08,718 \n",
      "MICRO_AVG: acc 0.5292 - f1-score 0.6921\n",
      "MACRO_AVG: acc 0.5273 - f1-score 0.69\n",
      "0          tp: 3880 - fp: 1968 - fn: 1110 - tn: 3043 - precision: 0.6635 - recall: 0.7776 - accuracy: 0.5576 - f1-score: 0.7160\n",
      "1          tp: 3042 - fp: 1111 - fn: 1968 - tn: 3880 - precision: 0.7325 - recall: 0.6072 - accuracy: 0.4970 - f1-score: 0.6640\n",
      "2019-10-19 13:43:08,719 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:44:16,564 Reading data from weighted_sampling\n",
      "2019-10-19 13:44:16,565 Train: weighted_sampling/labelled_128_1571492656.csv\n",
      "2019-10-19 13:44:16,566 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 13:44:16,567 Test: weighted_sampling/test.csv\n",
      "2019-10-19 13:44:16,646 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:16,647 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:44:16,649 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:16,650 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:44:16,651 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:16,652 Parameters:\n",
      "2019-10-19 13:44:16,652  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:44:16,653  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:44:16,655  - patience: \"5\"\n",
      "2019-10-19 13:44:16,656  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:44:16,657  - max_epochs: \"1\"\n",
      "2019-10-19 13:44:16,657  - shuffle: \"True\"\n",
      "2019-10-19 13:44:16,658  - train_with_dev: \"False\"\n",
      "2019-10-19 13:44:16,659 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:16,660 Model training base path: \"weighted_sampling/2\"\n",
      "2019-10-19 13:44:16,665 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:16,665 Device: cuda:0\n",
      "2019-10-19 13:44:16,666 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:16,667 Embeddings storage mode: gpu\n",
      "2019-10-19 13:44:16,671 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:17,531 epoch 1 - iter 0/4 - loss 0.99237704 - samples/sec: 124.54\n",
      "2019-10-19 13:44:17,783 epoch 1 - iter 1/4 - loss 1.00652313 - samples/sec: 140.10\n",
      "2019-10-19 13:44:18,033 epoch 1 - iter 2/4 - loss 0.93991188 - samples/sec: 137.41\n",
      "2019-10-19 13:44:18,283 epoch 1 - iter 3/4 - loss 0.96904907 - samples/sec: 140.64\n",
      "2019-10-19 13:44:18,526 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:44:18,528 EPOCH 1 done: loss 0.9690 - lr 0.0000\n",
      "2019-10-19 13:45:28,548 DEV : loss 0.7786562442779541 - score 0.6761\n",
      "2019-10-19 13:45:31,657 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:45:41,478 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:45:41,480 Testing using best model ...\n",
      "2019-10-19 13:45:41,482 loading file weighted_sampling/2/best-model.pt\n",
      "2019-10-19 13:47:14,530 0.6815\t0.6816\t0.6815\n",
      "2019-10-19 13:47:14,532 \n",
      "MICRO_AVG: acc 0.517 - f1-score 0.6815\n",
      "MACRO_AVG: acc 0.5118 - f1-score 0.67555\n",
      "0          tp: 4088 - fp: 2282 - fn: 902 - tn: 2729 - precision: 0.6418 - recall: 0.8192 - accuracy: 0.5622 - f1-score: 0.7197\n",
      "1          tp: 2728 - fp: 903 - fn: 2282 - tn: 4088 - precision: 0.7513 - recall: 0.5445 - accuracy: 0.4614 - f1-score: 0.6314\n",
      "2019-10-19 13:47:14,533 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:48:15,940 Reading data from weighted_sampling\n",
      "2019-10-19 13:48:15,941 Train: weighted_sampling/labelled_128_1571492895.csv\n",
      "2019-10-19 13:48:15,941 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 13:48:15,942 Test: weighted_sampling/test.csv\n",
      "2019-10-19 13:48:15,992 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:15,994 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:48:15,994 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:15,995 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:48:15,996 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:15,997 Parameters:\n",
      "2019-10-19 13:48:15,998  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:48:15,999  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:48:16,000  - patience: \"5\"\n",
      "2019-10-19 13:48:16,000  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:48:16,001  - max_epochs: \"1\"\n",
      "2019-10-19 13:48:16,002  - shuffle: \"True\"\n",
      "2019-10-19 13:48:16,002  - train_with_dev: \"False\"\n",
      "2019-10-19 13:48:16,003 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:16,003 Model training base path: \"weighted_sampling/3\"\n",
      "2019-10-19 13:48:16,004 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:16,004 Device: cuda:0\n",
      "2019-10-19 13:48:16,005 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:16,006 Embeddings storage mode: gpu\n",
      "2019-10-19 13:48:16,009 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:16,811 epoch 1 - iter 0/4 - loss 1.08983600 - samples/sec: 132.76\n",
      "2019-10-19 13:48:17,067 epoch 1 - iter 1/4 - loss 1.00542647 - samples/sec: 137.21\n",
      "2019-10-19 13:48:17,339 epoch 1 - iter 2/4 - loss 0.86918875 - samples/sec: 131.94\n",
      "2019-10-19 13:48:17,583 epoch 1 - iter 3/4 - loss 0.87601192 - samples/sec: 141.61\n",
      "2019-10-19 13:48:17,834 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:48:17,835 EPOCH 1 done: loss 0.8760 - lr 0.0000\n",
      "2019-10-19 13:49:36,122 DEV : loss 0.8286798596382141 - score 0.6628\n",
      "2019-10-19 13:49:40,117 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:49:50,372 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:49:50,374 Testing using best model ...\n",
      "2019-10-19 13:49:50,375 loading file weighted_sampling/3/best-model.pt\n",
      "2019-10-19 13:51:03,076 0.6705\t0.6706\t0.6705\n",
      "2019-10-19 13:51:03,079 \n",
      "MICRO_AVG: acc 0.5044 - f1-score 0.6705\n",
      "MACRO_AVG: acc 0.4951 - f1-score 0.6595\n",
      "0          tp: 4254 - fp: 2558 - fn: 736 - tn: 2453 - precision: 0.6245 - recall: 0.8525 - accuracy: 0.5636 - f1-score: 0.7209\n",
      "1          tp: 2452 - fp: 737 - fn: 2558 - tn: 4254 - precision: 0.7689 - recall: 0.4894 - accuracy: 0.4267 - f1-score: 0.5981\n",
      "2019-10-19 13:51:03,080 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:52:07,341 Reading data from weighted_sampling\n",
      "2019-10-19 13:52:07,342 Train: weighted_sampling/labelled_128_1571493127.csv\n",
      "2019-10-19 13:52:07,343 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 13:52:07,344 Test: weighted_sampling/test.csv\n",
      "2019-10-19 13:52:07,396 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:07,397 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:52:07,398 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:07,399 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:52:07,400 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:07,400 Parameters:\n",
      "2019-10-19 13:52:07,401  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:52:07,401  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:52:07,402  - patience: \"5\"\n",
      "2019-10-19 13:52:07,403  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:52:07,404  - max_epochs: \"1\"\n",
      "2019-10-19 13:52:07,404  - shuffle: \"True\"\n",
      "2019-10-19 13:52:07,405  - train_with_dev: \"False\"\n",
      "2019-10-19 13:52:07,406 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:07,406 Model training base path: \"weighted_sampling/4\"\n",
      "2019-10-19 13:52:07,407 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:07,408 Device: cuda:0\n",
      "2019-10-19 13:52:07,408 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:07,409 Embeddings storage mode: gpu\n",
      "2019-10-19 13:52:07,412 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:08,409 epoch 1 - iter 0/4 - loss 0.76018673 - samples/sec: 135.44\n",
      "2019-10-19 13:52:08,653 epoch 1 - iter 1/4 - loss 0.74495280 - samples/sec: 142.54\n",
      "2019-10-19 13:52:08,894 epoch 1 - iter 2/4 - loss 0.73765494 - samples/sec: 143.56\n",
      "2019-10-19 13:52:09,152 epoch 1 - iter 3/4 - loss 0.78127925 - samples/sec: 132.53\n",
      "2019-10-19 13:52:09,438 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:52:09,440 EPOCH 1 done: loss 0.7813 - lr 0.0000\n",
      "2019-10-19 13:53:25,309 DEV : loss 0.8918876051902771 - score 0.644\n",
      "2019-10-19 13:53:29,197 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:53:39,882 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:53:39,883 Testing using best model ...\n",
      "2019-10-19 13:53:39,884 loading file weighted_sampling/4/best-model.pt\n",
      "2019-10-19 13:54:59,781 0.6588\t0.6589\t0.6588\n",
      "2019-10-19 13:54:59,786 \n",
      "MICRO_AVG: acc 0.4913 - f1-score 0.6588\n",
      "MACRO_AVG: acc 0.4762 - f1-score 0.6402\n",
      "0          tp: 4432 - fp: 2853 - fn: 558 - tn: 2158 - precision: 0.6084 - recall: 0.8882 - accuracy: 0.5651 - f1-score: 0.7221\n",
      "1          tp: 2157 - fp: 559 - fn: 2853 - tn: 4432 - precision: 0.7942 - recall: 0.4305 - accuracy: 0.3873 - f1-score: 0.5583\n",
      "2019-10-19 13:54:59,789 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:56:04,585 Reading data from weighted_sampling\n",
      "2019-10-19 13:56:04,587 Train: weighted_sampling/labelled_128_1571493364.csv\n",
      "2019-10-19 13:56:04,587 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 13:56:04,588 Test: weighted_sampling/test.csv\n",
      "2019-10-19 13:56:04,674 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:04,675 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:56:04,675 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:04,676 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:56:04,677 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:04,678 Parameters:\n",
      "2019-10-19 13:56:04,678  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:56:04,679  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:56:04,682  - patience: \"5\"\n",
      "2019-10-19 13:56:04,682  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:56:04,683  - max_epochs: \"1\"\n",
      "2019-10-19 13:56:04,684  - shuffle: \"True\"\n",
      "2019-10-19 13:56:04,686  - train_with_dev: \"False\"\n",
      "2019-10-19 13:56:04,687 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:04,688 Model training base path: \"weighted_sampling/5\"\n",
      "2019-10-19 13:56:04,688 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:04,689 Device: cuda:0\n",
      "2019-10-19 13:56:04,690 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:04,691 Embeddings storage mode: gpu\n",
      "2019-10-19 13:56:04,695 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:05,613 epoch 1 - iter 0/4 - loss 0.75641561 - samples/sec: 123.61\n",
      "2019-10-19 13:56:05,867 epoch 1 - iter 1/4 - loss 0.72266549 - samples/sec: 147.47\n",
      "2019-10-19 13:56:06,124 epoch 1 - iter 2/4 - loss 0.73333204 - samples/sec: 136.71\n",
      "2019-10-19 13:56:06,378 epoch 1 - iter 3/4 - loss 0.67594360 - samples/sec: 136.66\n",
      "2019-10-19 13:56:06,670 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:56:06,672 EPOCH 1 done: loss 0.6759 - lr 0.0000\n",
      "2019-10-19 13:57:16,582 DEV : loss 0.966594934463501 - score 0.6299\n",
      "2019-10-19 13:57:20,048 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 13:57:28,423 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:57:28,424 Testing using best model ...\n",
      "2019-10-19 13:57:28,425 loading file weighted_sampling/5/best-model.pt\n",
      "2019-10-19 13:58:46,297 0.6464\t0.6465\t0.6464\n",
      "2019-10-19 13:58:46,299 \n",
      "MICRO_AVG: acc 0.4776 - f1-score 0.6464\n",
      "MACRO_AVG: acc 0.4563 - f1-score 0.6190500000000001\n",
      "0          tp: 4574 - fp: 3119 - fn: 416 - tn: 1892 - precision: 0.5946 - recall: 0.9166 - accuracy: 0.5641 - f1-score: 0.7213\n",
      "1          tp: 1891 - fp: 417 - fn: 3119 - tn: 4574 - precision: 0.8193 - recall: 0.3774 - accuracy: 0.3484 - f1-score: 0.5168\n",
      "2019-10-19 13:58:46,301 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 13:59:50,086 Reading data from weighted_sampling\n",
      "2019-10-19 13:59:50,088 Train: weighted_sampling/labelled_128_1571493590.csv\n",
      "2019-10-19 13:59:50,089 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 13:59:50,090 Test: weighted_sampling/test.csv\n",
      "2019-10-19 13:59:50,166 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:50,168 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 13:59:50,169 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:50,170 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 13:59:50,171 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:50,171 Parameters:\n",
      "2019-10-19 13:59:50,172  - learning_rate: \"1e-05\"\n",
      "2019-10-19 13:59:50,173  - mini_batch_size: \"32\"\n",
      "2019-10-19 13:59:50,174  - patience: \"5\"\n",
      "2019-10-19 13:59:50,174  - anneal_factor: \"0.5\"\n",
      "2019-10-19 13:59:50,175  - max_epochs: \"1\"\n",
      "2019-10-19 13:59:50,176  - shuffle: \"True\"\n",
      "2019-10-19 13:59:50,177  - train_with_dev: \"False\"\n",
      "2019-10-19 13:59:50,178 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:50,179 Model training base path: \"weighted_sampling/6\"\n",
      "2019-10-19 13:59:50,180 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:50,180 Device: cuda:0\n",
      "2019-10-19 13:59:50,181 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:50,182 Embeddings storage mode: gpu\n",
      "2019-10-19 13:59:50,189 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:51,072 epoch 1 - iter 0/4 - loss 0.69250751 - samples/sec: 137.70\n",
      "2019-10-19 13:59:51,313 epoch 1 - iter 1/4 - loss 0.74752545 - samples/sec: 147.49\n",
      "2019-10-19 13:59:51,570 epoch 1 - iter 2/4 - loss 0.64351512 - samples/sec: 137.07\n",
      "2019-10-19 13:59:51,792 epoch 1 - iter 3/4 - loss 0.61394931 - samples/sec: 163.45\n",
      "2019-10-19 13:59:52,085 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 13:59:52,087 EPOCH 1 done: loss 0.6139 - lr 0.0000\n",
      "2019-10-19 14:01:08,335 DEV : loss 1.0516915321350098 - score 0.6107\n",
      "2019-10-19 14:01:11,475 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:01:20,503 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:01:20,505 Testing using best model ...\n",
      "2019-10-19 14:01:20,506 loading file weighted_sampling/6/best-model.pt\n",
      "2019-10-19 14:02:32,633 0.6268\t0.6269\t0.6268\n",
      "2019-10-19 14:02:32,636 \n",
      "MICRO_AVG: acc 0.4565 - f1-score 0.6268\n",
      "MACRO_AVG: acc 0.4284 - f1-score 0.58885\n",
      "0          tp: 4656 - fp: 3397 - fn: 334 - tn: 1614 - precision: 0.5782 - recall: 0.9331 - accuracy: 0.5551 - f1-score: 0.7140\n",
      "1          tp: 1613 - fp: 335 - fn: 3397 - tn: 4656 - precision: 0.8280 - recall: 0.3220 - accuracy: 0.3018 - f1-score: 0.4637\n",
      "2019-10-19 14:02:32,637 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:03:43,270 Reading data from weighted_sampling\n",
      "2019-10-19 14:03:43,272 Train: weighted_sampling/labelled_128_1571493823.csv\n",
      "2019-10-19 14:03:43,273 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:03:43,273 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:03:43,347 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:43,350 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:03:43,351 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:43,353 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:03:43,353 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:43,355 Parameters:\n",
      "2019-10-19 14:03:43,355  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:03:43,356  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:03:43,357  - patience: \"5\"\n",
      "2019-10-19 14:03:43,359  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:03:43,359  - max_epochs: \"1\"\n",
      "2019-10-19 14:03:43,360  - shuffle: \"True\"\n",
      "2019-10-19 14:03:43,361  - train_with_dev: \"False\"\n",
      "2019-10-19 14:03:43,362 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:43,362 Model training base path: \"weighted_sampling/7\"\n",
      "2019-10-19 14:03:43,363 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:43,364 Device: cuda:0\n",
      "2019-10-19 14:03:43,364 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:43,365 Embeddings storage mode: gpu\n",
      "2019-10-19 14:03:43,369 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:44,282 epoch 1 - iter 0/4 - loss 0.83856618 - samples/sec: 115.23\n",
      "2019-10-19 14:03:44,518 epoch 1 - iter 1/4 - loss 0.67914167 - samples/sec: 151.51\n",
      "2019-10-19 14:03:44,765 epoch 1 - iter 2/4 - loss 0.57519428 - samples/sec: 147.40\n",
      "2019-10-19 14:03:44,994 epoch 1 - iter 3/4 - loss 0.53769185 - samples/sec: 150.88\n",
      "2019-10-19 14:03:45,314 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:03:45,316 EPOCH 1 done: loss 0.5377 - lr 0.0000\n",
      "2019-10-19 14:04:55,607 DEV : loss 1.1473138332366943 - score 0.5958\n",
      "2019-10-19 14:04:59,480 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:05:08,752 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:05:08,755 Testing using best model ...\n",
      "2019-10-19 14:05:08,756 loading file weighted_sampling/7/best-model.pt\n",
      "2019-10-19 14:06:28,833 0.6093\t0.6094\t0.6093\n",
      "2019-10-19 14:06:28,835 \n",
      "MICRO_AVG: acc 0.4382 - f1-score 0.6093\n",
      "MACRO_AVG: acc 0.4032 - f1-score 0.5593\n",
      "0          tp: 4733 - fp: 3649 - fn: 257 - tn: 1362 - precision: 0.5647 - recall: 0.9485 - accuracy: 0.5479 - f1-score: 0.7079\n",
      "1          tp: 1361 - fp: 258 - fn: 3649 - tn: 4733 - precision: 0.8406 - recall: 0.2717 - accuracy: 0.2584 - f1-score: 0.4107\n",
      "2019-10-19 14:06:28,836 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:07:31,258 Reading data from weighted_sampling\n",
      "2019-10-19 14:07:31,265 Train: weighted_sampling/labelled_128_1571494051.csv\n",
      "2019-10-19 14:07:31,266 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:07:31,266 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:07:31,324 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:31,325 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:07:31,326 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:31,327 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:07:31,327 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:31,328 Parameters:\n",
      "2019-10-19 14:07:31,329  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:07:31,330  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:07:31,330  - patience: \"5\"\n",
      "2019-10-19 14:07:31,331  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:07:31,333  - max_epochs: \"1\"\n",
      "2019-10-19 14:07:31,333  - shuffle: \"True\"\n",
      "2019-10-19 14:07:31,334  - train_with_dev: \"False\"\n",
      "2019-10-19 14:07:31,335 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:31,335 Model training base path: \"weighted_sampling/8\"\n",
      "2019-10-19 14:07:31,337 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:31,338 Device: cuda:0\n",
      "2019-10-19 14:07:31,339 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:31,340 Embeddings storage mode: gpu\n",
      "2019-10-19 14:07:31,344 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:32,162 epoch 1 - iter 0/4 - loss 0.60365450 - samples/sec: 149.77\n",
      "2019-10-19 14:07:32,436 epoch 1 - iter 1/4 - loss 0.62367555 - samples/sec: 132.58\n",
      "2019-10-19 14:07:32,672 epoch 1 - iter 2/4 - loss 0.60829498 - samples/sec: 143.82\n",
      "2019-10-19 14:07:32,941 epoch 1 - iter 3/4 - loss 0.58643098 - samples/sec: 126.58\n",
      "2019-10-19 14:07:33,221 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:07:33,222 EPOCH 1 done: loss 0.5864 - lr 0.0000\n",
      "2019-10-19 14:08:50,165 DEV : loss 1.25194251537323 - score 0.5822\n",
      "2019-10-19 14:08:54,067 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:09:03,046 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:09:03,047 Testing using best model ...\n",
      "2019-10-19 14:09:03,049 loading file weighted_sampling/8/best-model.pt\n",
      "2019-10-19 14:10:15,408 0.5954\t0.5955\t0.5954\n",
      "2019-10-19 14:10:15,409 \n",
      "MICRO_AVG: acc 0.424 - f1-score 0.5954\n",
      "MACRO_AVG: acc 0.383 - f1-score 0.5345\n",
      "0          tp: 4787 - fp: 3842 - fn: 203 - tn: 1169 - precision: 0.5548 - recall: 0.9593 - accuracy: 0.5420 - f1-score: 0.7030\n",
      "1          tp: 1168 - fp: 204 - fn: 3842 - tn: 4787 - precision: 0.8513 - recall: 0.2331 - accuracy: 0.2240 - f1-score: 0.3660\n",
      "2019-10-19 14:10:15,410 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:11:21,007 Reading data from weighted_sampling\n",
      "2019-10-19 14:11:21,015 Train: weighted_sampling/labelled_128_1571494280.csv\n",
      "2019-10-19 14:11:21,016 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:11:21,016 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:11:21,160 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:21,163 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:11:21,164 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:21,165 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:11:21,166 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:21,167 Parameters:\n",
      "2019-10-19 14:11:21,168  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:11:21,169  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:11:21,169  - patience: \"5\"\n",
      "2019-10-19 14:11:21,170  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:11:21,171  - max_epochs: \"1\"\n",
      "2019-10-19 14:11:21,172  - shuffle: \"True\"\n",
      "2019-10-19 14:11:21,173  - train_with_dev: \"False\"\n",
      "2019-10-19 14:11:21,174 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:21,175 Model training base path: \"weighted_sampling/9\"\n",
      "2019-10-19 14:11:21,176 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:21,177 Device: cuda:0\n",
      "2019-10-19 14:11:21,178 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:21,179 Embeddings storage mode: gpu\n",
      "2019-10-19 14:11:21,182 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:22,557 epoch 1 - iter 0/4 - loss 0.57717502 - samples/sec: 106.66\n",
      "2019-10-19 14:11:22,939 epoch 1 - iter 1/4 - loss 0.63991031 - samples/sec: 98.23\n",
      "2019-10-19 14:11:23,218 epoch 1 - iter 2/4 - loss 0.60752112 - samples/sec: 129.44\n",
      "2019-10-19 14:11:23,508 epoch 1 - iter 3/4 - loss 0.59523572 - samples/sec: 126.54\n",
      "2019-10-19 14:11:23,947 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:11:23,949 EPOCH 1 done: loss 0.5952 - lr 0.0000\n",
      "2019-10-19 14:17:52,338 DEV : loss 1.3654757738113403 - score 0.5692\n",
      "2019-10-19 14:17:54,557 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:18:01,056 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:18:01,058 Testing using best model ...\n",
      "2019-10-19 14:18:01,059 loading file weighted_sampling/9/best-model.pt\n",
      "2019-10-19 14:19:18,122 0.5808\t0.5809\t0.5808\n",
      "2019-10-19 14:19:18,124 \n",
      "MICRO_AVG: acc 0.4093 - f1-score 0.5808\n",
      "MACRO_AVG: acc 0.3624 - f1-score 0.508\n",
      "0          tp: 4829 - fp: 4030 - fn: 161 - tn: 981 - precision: 0.5451 - recall: 0.9677 - accuracy: 0.5354 - f1-score: 0.6974\n",
      "1          tp: 980 - fp: 162 - fn: 4030 - tn: 4829 - precision: 0.8581 - recall: 0.1956 - accuracy: 0.1895 - f1-score: 0.3186\n",
      "2019-10-19 14:19:18,124 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:20:16,216 Reading data from weighted_sampling\n",
      "2019-10-19 14:20:16,218 Train: weighted_sampling/labelled_128_1571494816.csv\n",
      "2019-10-19 14:20:16,218 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:20:16,219 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:20:16,408 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:16,410 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:20:16,411 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:16,411 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:20:16,412 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:16,412 Parameters:\n",
      "2019-10-19 14:20:16,413  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:20:16,414  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:20:16,414  - patience: \"5\"\n",
      "2019-10-19 14:20:16,415  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:20:16,415  - max_epochs: \"1\"\n",
      "2019-10-19 14:20:16,416  - shuffle: \"True\"\n",
      "2019-10-19 14:20:16,416  - train_with_dev: \"False\"\n",
      "2019-10-19 14:20:16,417 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:16,418 Model training base path: \"weighted_sampling/10\"\n",
      "2019-10-19 14:20:16,418 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:16,419 Device: cuda:0\n",
      "2019-10-19 14:20:16,419 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:16,420 Embeddings storage mode: gpu\n",
      "2019-10-19 14:20:16,425 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:17,276 epoch 1 - iter 0/4 - loss 0.51989526 - samples/sec: 103.16\n",
      "2019-10-19 14:20:17,500 epoch 1 - iter 1/4 - loss 0.60099211 - samples/sec: 156.25\n",
      "2019-10-19 14:20:17,741 epoch 1 - iter 2/4 - loss 0.55134197 - samples/sec: 142.38\n",
      "2019-10-19 14:20:17,987 epoch 1 - iter 3/4 - loss 0.56940037 - samples/sec: 140.22\n",
      "2019-10-19 14:20:18,261 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:20:18,263 EPOCH 1 done: loss 0.5694 - lr 0.0000\n",
      "2019-10-19 14:21:26,700 DEV : loss 1.4862902164459229 - score 0.5567\n",
      "2019-10-19 14:21:28,894 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:21:35,558 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:21:35,559 Testing using best model ...\n",
      "2019-10-19 14:21:35,560 loading file weighted_sampling/10/best-model.pt\n",
      "2019-10-19 14:22:51,822 0.568\t0.5681\t0.568\n",
      "2019-10-19 14:22:51,824 \n",
      "MICRO_AVG: acc 0.3967 - f1-score 0.568\n",
      "MACRO_AVG: acc 0.3441 - f1-score 0.4829\n",
      "0          tp: 4870 - fp: 4199 - fn: 120 - tn: 812 - precision: 0.5370 - recall: 0.9760 - accuracy: 0.5300 - f1-score: 0.6928\n",
      "1          tp: 811 - fp: 121 - fn: 4199 - tn: 4870 - precision: 0.8702 - recall: 0.1619 - accuracy: 0.1581 - f1-score: 0.2730\n",
      "2019-10-19 14:22:51,825 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:23:49,923 Reading data from weighted_sampling\n",
      "2019-10-19 14:23:49,924 Train: weighted_sampling/labelled_128_1571495029.csv\n",
      "2019-10-19 14:23:49,925 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:23:49,926 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:23:49,975 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:49,977 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:23:49,977 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:49,978 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:23:49,979 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:49,980 Parameters:\n",
      "2019-10-19 14:23:49,980  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:23:49,981  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:23:49,982  - patience: \"5\"\n",
      "2019-10-19 14:23:49,982  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:23:49,983  - max_epochs: \"1\"\n",
      "2019-10-19 14:23:49,984  - shuffle: \"True\"\n",
      "2019-10-19 14:23:49,984  - train_with_dev: \"False\"\n",
      "2019-10-19 14:23:49,985 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:49,985 Model training base path: \"weighted_sampling/11\"\n",
      "2019-10-19 14:23:49,986 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:49,987 Device: cuda:0\n",
      "2019-10-19 14:23:49,987 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:49,988 Embeddings storage mode: gpu\n",
      "2019-10-19 14:23:49,993 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:50,693 epoch 1 - iter 0/4 - loss 0.46147895 - samples/sec: 190.15\n",
      "2019-10-19 14:23:50,942 epoch 1 - iter 1/4 - loss 0.41335751 - samples/sec: 138.17\n",
      "2019-10-19 14:23:51,207 epoch 1 - iter 2/4 - loss 0.39960931 - samples/sec: 139.50\n",
      "2019-10-19 14:23:51,414 epoch 1 - iter 3/4 - loss 0.39152128 - samples/sec: 165.35\n",
      "2019-10-19 14:23:51,703 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:23:51,704 EPOCH 1 done: loss 0.3915 - lr 0.0000\n",
      "2019-10-19 14:25:06,065 DEV : loss 1.611467719078064 - score 0.5474\n",
      "2019-10-19 14:25:08,308 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:25:15,158 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:25:15,159 Testing using best model ...\n",
      "2019-10-19 14:25:15,161 loading file weighted_sampling/11/best-model.pt\n",
      "2019-10-19 14:26:24,882 0.5557\t0.5558\t0.5557\n",
      "2019-10-19 14:26:24,884 \n",
      "MICRO_AVG: acc 0.3848 - f1-score 0.5557\n",
      "MACRO_AVG: acc 0.3273 - f1-score 0.4593\n",
      "0          tp: 4891 - fp: 4343 - fn: 99 - tn: 668 - precision: 0.5297 - recall: 0.9802 - accuracy: 0.5241 - f1-score: 0.6877\n",
      "1          tp: 667 - fp: 100 - fn: 4343 - tn: 4891 - precision: 0.8696 - recall: 0.1331 - accuracy: 0.1305 - f1-score: 0.2309\n",
      "2019-10-19 14:26:24,884 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:27:27,529 Reading data from weighted_sampling\n",
      "2019-10-19 14:27:27,530 Train: weighted_sampling/labelled_128_1571495247.csv\n",
      "2019-10-19 14:27:27,531 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:27:27,532 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:27:27,572 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:27,574 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:27:27,574 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:27,575 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:27:27,576 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:27,576 Parameters:\n",
      "2019-10-19 14:27:27,577  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:27:27,578  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:27:27,578  - patience: \"5\"\n",
      "2019-10-19 14:27:27,579  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:27:27,580  - max_epochs: \"1\"\n",
      "2019-10-19 14:27:27,580  - shuffle: \"True\"\n",
      "2019-10-19 14:27:27,581  - train_with_dev: \"False\"\n",
      "2019-10-19 14:27:27,582 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:27,582 Model training base path: \"weighted_sampling/12\"\n",
      "2019-10-19 14:27:27,583 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:27,584 Device: cuda:0\n",
      "2019-10-19 14:27:27,584 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:27,585 Embeddings storage mode: gpu\n",
      "2019-10-19 14:27:27,590 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:28,281 epoch 1 - iter 0/4 - loss 0.54321283 - samples/sec: 132.52\n",
      "2019-10-19 14:27:28,523 epoch 1 - iter 1/4 - loss 0.49942435 - samples/sec: 144.73\n",
      "2019-10-19 14:27:28,755 epoch 1 - iter 2/4 - loss 0.48118195 - samples/sec: 149.70\n",
      "2019-10-19 14:27:28,953 epoch 1 - iter 3/4 - loss 0.45901940 - samples/sec: 172.09\n",
      "2019-10-19 14:27:29,160 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:27:29,161 EPOCH 1 done: loss 0.4590 - lr 0.0000\n",
      "2019-10-19 14:28:37,286 DEV : loss 1.7423251867294312 - score 0.5382\n",
      "2019-10-19 14:28:39,504 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:28:46,263 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:28:46,265 Testing using best model ...\n",
      "2019-10-19 14:28:46,266 loading file weighted_sampling/12/best-model.pt\n",
      "2019-10-19 14:30:02,555 0.5453\t0.5454\t0.5453\n",
      "2019-10-19 14:30:02,557 \n",
      "MICRO_AVG: acc 0.3749 - f1-score 0.5453\n",
      "MACRO_AVG: acc 0.3127 - f1-score 0.4376\n",
      "0          tp: 4916 - fp: 4472 - fn: 74 - tn: 539 - precision: 0.5236 - recall: 0.9852 - accuracy: 0.5196 - f1-score: 0.6838\n",
      "1          tp: 538 - fp: 75 - fn: 4472 - tn: 4916 - precision: 0.8777 - recall: 0.1074 - accuracy: 0.1058 - f1-score: 0.1914\n",
      "2019-10-19 14:30:02,558 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:31:01,135 Reading data from weighted_sampling\n",
      "2019-10-19 14:31:01,137 Train: weighted_sampling/labelled_128_1571495461.csv\n",
      "2019-10-19 14:31:01,138 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:31:01,138 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:31:01,193 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:01,194 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:31:01,195 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:01,196 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:31:01,196 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:01,197 Parameters:\n",
      "2019-10-19 14:31:01,198  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:31:01,198  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:31:01,199  - patience: \"5\"\n",
      "2019-10-19 14:31:01,200  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:31:01,200  - max_epochs: \"1\"\n",
      "2019-10-19 14:31:01,203  - shuffle: \"True\"\n",
      "2019-10-19 14:31:01,203  - train_with_dev: \"False\"\n",
      "2019-10-19 14:31:01,204 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:01,204 Model training base path: \"weighted_sampling/13\"\n",
      "2019-10-19 14:31:01,205 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:01,206 Device: cuda:0\n",
      "2019-10-19 14:31:01,206 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:01,207 Embeddings storage mode: gpu\n",
      "2019-10-19 14:31:01,210 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:01,900 epoch 1 - iter 0/4 - loss 0.48539126 - samples/sec: 149.29\n",
      "2019-10-19 14:31:02,134 epoch 1 - iter 1/4 - loss 0.45051064 - samples/sec: 157.31\n",
      "2019-10-19 14:31:02,358 epoch 1 - iter 2/4 - loss 0.44275369 - samples/sec: 161.53\n",
      "2019-10-19 14:31:02,626 epoch 1 - iter 3/4 - loss 0.44963156 - samples/sec: 129.53\n",
      "2019-10-19 14:31:02,883 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:31:02,885 EPOCH 1 done: loss 0.4496 - lr 0.0000\n",
      "2019-10-19 14:32:18,022 DEV : loss 1.877880573272705 - score 0.5313\n",
      "2019-10-19 14:32:20,324 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:32:27,042 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:32:27,044 Testing using best model ...\n",
      "2019-10-19 14:32:27,045 loading file weighted_sampling/13/best-model.pt\n",
      "2019-10-19 14:33:37,065 0.5378\t0.5379\t0.5378\n",
      "2019-10-19 14:33:37,066 \n",
      "MICRO_AVG: acc 0.3679 - f1-score 0.5378\n",
      "MACRO_AVG: acc 0.3021 - f1-score 0.42135\n",
      "0          tp: 4933 - fp: 4564 - fn: 57 - tn: 447 - precision: 0.5194 - recall: 0.9886 - accuracy: 0.5163 - f1-score: 0.6810\n",
      "1          tp: 446 - fp: 58 - fn: 4564 - tn: 4933 - precision: 0.8849 - recall: 0.0890 - accuracy: 0.0880 - f1-score: 0.1617\n",
      "2019-10-19 14:33:37,067 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:34:34,390 Reading data from weighted_sampling\n",
      "2019-10-19 14:34:34,391 Train: weighted_sampling/labelled_128_1571495674.csv\n",
      "2019-10-19 14:34:34,392 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:34:34,392 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:34:34,437 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:34,438 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:34:34,438 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:34,439 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:34:34,440 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:34,440 Parameters:\n",
      "2019-10-19 14:34:34,441  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:34:34,442  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:34:34,442  - patience: \"5\"\n",
      "2019-10-19 14:34:34,443  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:34:34,444  - max_epochs: \"1\"\n",
      "2019-10-19 14:34:34,445  - shuffle: \"True\"\n",
      "2019-10-19 14:34:34,445  - train_with_dev: \"False\"\n",
      "2019-10-19 14:34:34,446 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:34,447 Model training base path: \"weighted_sampling/14\"\n",
      "2019-10-19 14:34:34,447 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:34,448 Device: cuda:0\n",
      "2019-10-19 14:34:34,449 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:34,449 Embeddings storage mode: gpu\n",
      "2019-10-19 14:34:34,456 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:35,196 epoch 1 - iter 0/4 - loss 0.51386982 - samples/sec: 140.01\n",
      "2019-10-19 14:34:35,413 epoch 1 - iter 1/4 - loss 0.43787462 - samples/sec: 159.95\n",
      "2019-10-19 14:34:35,605 epoch 1 - iter 2/4 - loss 0.41558824 - samples/sec: 180.40\n",
      "2019-10-19 14:34:35,836 epoch 1 - iter 3/4 - loss 0.41576457 - samples/sec: 150.55\n",
      "2019-10-19 14:34:36,075 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:34:36,077 EPOCH 1 done: loss 0.4158 - lr 0.0000\n",
      "2019-10-19 14:35:49,791 DEV : loss 2.0178632736206055 - score 0.5242\n",
      "2019-10-19 14:35:52,040 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:35:58,758 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:35:58,760 Testing using best model ...\n",
      "2019-10-19 14:35:58,761 loading file weighted_sampling/14/best-model.pt\n",
      "2019-10-19 14:37:15,486 0.5298\t0.5299\t0.5298\n",
      "2019-10-19 14:37:15,488 \n",
      "MICRO_AVG: acc 0.3604 - f1-score 0.5298\n",
      "MACRO_AVG: acc 0.2908 - f1-score 0.40340000000000004\n",
      "0          tp: 4952 - fp: 4663 - fn: 38 - tn: 348 - precision: 0.5150 - recall: 0.9924 - accuracy: 0.5130 - f1-score: 0.6781\n",
      "1          tp: 347 - fp: 39 - fn: 4663 - tn: 4952 - precision: 0.8990 - recall: 0.0693 - accuracy: 0.0687 - f1-score: 0.1287\n",
      "2019-10-19 14:37:15,489 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:38:13,635 Reading data from weighted_sampling\n",
      "2019-10-19 14:38:13,637 Train: weighted_sampling/labelled_128_1571495893.csv\n",
      "2019-10-19 14:38:13,638 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:38:13,638 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:38:13,679 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:13,680 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:38:13,681 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:13,682 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:38:13,682 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:13,683 Parameters:\n",
      "2019-10-19 14:38:13,683  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:38:13,684  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:38:13,684  - patience: \"5\"\n",
      "2019-10-19 14:38:13,685  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:38:13,686  - max_epochs: \"1\"\n",
      "2019-10-19 14:38:13,686  - shuffle: \"True\"\n",
      "2019-10-19 14:38:13,687  - train_with_dev: \"False\"\n",
      "2019-10-19 14:38:13,687 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:13,688 Model training base path: \"weighted_sampling/15\"\n",
      "2019-10-19 14:38:13,688 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:13,689 Device: cuda:0\n",
      "2019-10-19 14:38:13,690 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:13,692 Embeddings storage mode: gpu\n",
      "2019-10-19 14:38:13,694 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:14,357 epoch 1 - iter 0/4 - loss 0.41556931 - samples/sec: 143.17\n",
      "2019-10-19 14:38:14,572 epoch 1 - iter 1/4 - loss 0.42226771 - samples/sec: 162.19\n",
      "2019-10-19 14:38:14,803 epoch 1 - iter 2/4 - loss 0.40327101 - samples/sec: 154.26\n",
      "2019-10-19 14:38:15,045 epoch 1 - iter 3/4 - loss 0.34969209 - samples/sec: 148.09\n",
      "2019-10-19 14:38:15,261 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:38:15,263 EPOCH 1 done: loss 0.3497 - lr 0.0000\n",
      "2019-10-19 14:39:23,228 DEV : loss 2.1627962589263916 - score 0.5183\n",
      "2019-10-19 14:39:25,439 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:39:32,604 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:39:32,605 Testing using best model ...\n",
      "2019-10-19 14:39:32,606 loading file weighted_sampling/15/best-model.pt\n",
      "2019-10-19 14:40:48,538 0.5234\t0.5235\t0.5234\n",
      "2019-10-19 14:40:48,540 \n",
      "MICRO_AVG: acc 0.3545 - f1-score 0.5234\n",
      "MACRO_AVG: acc 0.2823 - f1-score 0.38935\n",
      "0          tp: 4961 - fp: 4736 - fn: 29 - tn: 275 - precision: 0.5116 - recall: 0.9942 - accuracy: 0.5101 - f1-score: 0.6756\n",
      "1          tp: 274 - fp: 30 - fn: 4736 - tn: 4961 - precision: 0.9013 - recall: 0.0547 - accuracy: 0.0544 - f1-score: 0.1031\n",
      "2019-10-19 14:40:48,541 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:41:46,570 Reading data from weighted_sampling\n",
      "2019-10-19 14:41:46,572 Train: weighted_sampling/labelled_128_1571496106.csv\n",
      "2019-10-19 14:41:46,573 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:41:46,574 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:41:46,638 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:46,639 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:41:46,640 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:46,641 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:41:46,642 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:46,642 Parameters:\n",
      "2019-10-19 14:41:46,643  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:41:46,644  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:41:46,644  - patience: \"5\"\n",
      "2019-10-19 14:41:46,645  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:41:46,647  - max_epochs: \"1\"\n",
      "2019-10-19 14:41:46,648  - shuffle: \"True\"\n",
      "2019-10-19 14:41:46,648  - train_with_dev: \"False\"\n",
      "2019-10-19 14:41:46,649 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:46,649 Model training base path: \"weighted_sampling/16\"\n",
      "2019-10-19 14:41:46,650 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:46,651 Device: cuda:0\n",
      "2019-10-19 14:41:46,652 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:46,652 Embeddings storage mode: gpu\n",
      "2019-10-19 14:41:46,656 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:47,348 epoch 1 - iter 0/4 - loss 0.48480880 - samples/sec: 139.32\n",
      "2019-10-19 14:41:47,573 epoch 1 - iter 1/4 - loss 0.35449266 - samples/sec: 161.20\n",
      "2019-10-19 14:41:47,792 epoch 1 - iter 2/4 - loss 0.32611651 - samples/sec: 155.56\n",
      "2019-10-19 14:41:48,022 epoch 1 - iter 3/4 - loss 0.34707069 - samples/sec: 149.73\n",
      "2019-10-19 14:41:48,204 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:41:48,206 EPOCH 1 done: loss 0.3471 - lr 0.0000\n",
      "2019-10-19 14:43:02,721 DEV : loss 2.3083715438842773 - score 0.515\n",
      "2019-10-19 14:43:05,060 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:43:11,898 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:43:11,899 Testing using best model ...\n",
      "2019-10-19 14:43:11,900 loading file weighted_sampling/16/best-model.pt\n",
      "2019-10-19 14:44:21,774 0.5182\t0.5183\t0.5182\n",
      "2019-10-19 14:44:21,776 \n",
      "MICRO_AVG: acc 0.3498 - f1-score 0.5182\n",
      "MACRO_AVG: acc 0.2754 - f1-score 0.378\n",
      "0          tp: 4966 - fp: 4793 - fn: 24 - tn: 218 - precision: 0.5089 - recall: 0.9952 - accuracy: 0.5076 - f1-score: 0.6734\n",
      "1          tp: 217 - fp: 25 - fn: 4793 - tn: 4966 - precision: 0.8967 - recall: 0.0433 - accuracy: 0.0431 - f1-score: 0.0826\n",
      "2019-10-19 14:44:21,777 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:45:19,219 Reading data from weighted_sampling\n",
      "2019-10-19 14:45:19,228 Train: weighted_sampling/labelled_128_1571496319.csv\n",
      "2019-10-19 14:45:19,229 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:45:19,230 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:45:19,274 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:19,275 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:45:19,276 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:19,276 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:45:19,277 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:19,277 Parameters:\n",
      "2019-10-19 14:45:19,278  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:45:19,279  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:45:19,279  - patience: \"5\"\n",
      "2019-10-19 14:45:19,280  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:45:19,280  - max_epochs: \"1\"\n",
      "2019-10-19 14:45:19,281  - shuffle: \"True\"\n",
      "2019-10-19 14:45:19,281  - train_with_dev: \"False\"\n",
      "2019-10-19 14:45:19,282 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:19,282 Model training base path: \"weighted_sampling/17\"\n",
      "2019-10-19 14:45:19,283 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:19,284 Device: cuda:0\n",
      "2019-10-19 14:45:19,284 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:19,285 Embeddings storage mode: gpu\n",
      "2019-10-19 14:45:19,289 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:20,013 epoch 1 - iter 0/4 - loss 0.39379022 - samples/sec: 161.73\n",
      "2019-10-19 14:45:26,322 epoch 1 - iter 1/4 - loss 0.41150233 - samples/sec: 152.81\n",
      "2019-10-19 14:45:26,520 epoch 1 - iter 2/4 - loss 0.33935763 - samples/sec: 172.54\n",
      "2019-10-19 14:45:26,704 epoch 1 - iter 3/4 - loss 0.34614136 - samples/sec: 187.97\n",
      "2019-10-19 14:45:27,028 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:45:27,030 EPOCH 1 done: loss 0.3461 - lr 0.0000\n",
      "2019-10-19 14:46:35,243 DEV : loss 2.45521879196167 - score 0.5123\n",
      "2019-10-19 14:46:37,464 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:46:43,903 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:46:43,905 Testing using best model ...\n",
      "2019-10-19 14:46:43,906 loading file weighted_sampling/17/best-model.pt\n",
      "2019-10-19 14:48:01,055 0.5153\t0.5154\t0.5153\n",
      "2019-10-19 14:48:01,057 \n",
      "MICRO_AVG: acc 0.3471 - f1-score 0.5153\n",
      "MACRO_AVG: acc 0.2714 - f1-score 0.37135\n",
      "0          tp: 4970 - fp: 4826 - fn: 20 - tn: 185 - precision: 0.5073 - recall: 0.9960 - accuracy: 0.5063 - f1-score: 0.6722\n",
      "1          tp: 184 - fp: 21 - fn: 4826 - tn: 4970 - precision: 0.8976 - recall: 0.0367 - accuracy: 0.0366 - f1-score: 0.0705\n",
      "2019-10-19 14:48:01,057 ----------------------------------------------------------------------------------------------------\n",
      "STARTING LABELLING\n",
      "DONE 1/128 LABELS\n",
      "DONE 2/128 LABELS\n",
      "DONE 3/128 LABELS\n",
      "DONE 4/128 LABELS\n",
      "DONE 5/128 LABELS\n",
      "DONE 6/128 LABELS\n",
      "DONE 7/128 LABELS\n",
      "DONE 8/128 LABELS\n",
      "DONE 9/128 LABELS\n",
      "DONE 10/128 LABELS\n",
      "DONE 11/128 LABELS\n",
      "DONE 12/128 LABELS\n",
      "DONE 13/128 LABELS\n",
      "DONE 14/128 LABELS\n",
      "DONE 15/128 LABELS\n",
      "DONE 16/128 LABELS\n",
      "DONE 17/128 LABELS\n",
      "DONE 18/128 LABELS\n",
      "DONE 19/128 LABELS\n",
      "DONE 20/128 LABELS\n",
      "DONE 21/128 LABELS\n",
      "DONE 22/128 LABELS\n",
      "DONE 23/128 LABELS\n",
      "DONE 24/128 LABELS\n",
      "DONE 25/128 LABELS\n",
      "DONE 26/128 LABELS\n",
      "DONE 27/128 LABELS\n",
      "DONE 28/128 LABELS\n",
      "DONE 29/128 LABELS\n",
      "DONE 30/128 LABELS\n",
      "DONE 31/128 LABELS\n",
      "DONE 32/128 LABELS\n",
      "DONE 33/128 LABELS\n",
      "DONE 34/128 LABELS\n",
      "DONE 35/128 LABELS\n",
      "DONE 36/128 LABELS\n",
      "DONE 37/128 LABELS\n",
      "DONE 38/128 LABELS\n",
      "DONE 39/128 LABELS\n",
      "DONE 40/128 LABELS\n",
      "DONE 41/128 LABELS\n",
      "DONE 42/128 LABELS\n",
      "DONE 43/128 LABELS\n",
      "DONE 44/128 LABELS\n",
      "DONE 45/128 LABELS\n",
      "DONE 46/128 LABELS\n",
      "DONE 47/128 LABELS\n",
      "DONE 48/128 LABELS\n",
      "DONE 49/128 LABELS\n",
      "DONE 50/128 LABELS\n",
      "DONE 51/128 LABELS\n",
      "DONE 52/128 LABELS\n",
      "DONE 53/128 LABELS\n",
      "DONE 54/128 LABELS\n",
      "DONE 55/128 LABELS\n",
      "DONE 56/128 LABELS\n",
      "DONE 57/128 LABELS\n",
      "DONE 58/128 LABELS\n",
      "DONE 59/128 LABELS\n",
      "DONE 60/128 LABELS\n",
      "DONE 61/128 LABELS\n",
      "DONE 62/128 LABELS\n",
      "DONE 63/128 LABELS\n",
      "DONE 64/128 LABELS\n",
      "DONE 65/128 LABELS\n",
      "DONE 66/128 LABELS\n",
      "DONE 67/128 LABELS\n",
      "DONE 68/128 LABELS\n",
      "DONE 69/128 LABELS\n",
      "DONE 70/128 LABELS\n",
      "DONE 71/128 LABELS\n",
      "DONE 72/128 LABELS\n",
      "DONE 73/128 LABELS\n",
      "DONE 74/128 LABELS\n",
      "DONE 75/128 LABELS\n",
      "DONE 76/128 LABELS\n",
      "DONE 77/128 LABELS\n",
      "DONE 78/128 LABELS\n",
      "DONE 79/128 LABELS\n",
      "DONE 80/128 LABELS\n",
      "DONE 81/128 LABELS\n",
      "DONE 82/128 LABELS\n",
      "DONE 83/128 LABELS\n",
      "DONE 84/128 LABELS\n",
      "DONE 85/128 LABELS\n",
      "DONE 86/128 LABELS\n",
      "DONE 87/128 LABELS\n",
      "DONE 88/128 LABELS\n",
      "DONE 89/128 LABELS\n",
      "DONE 90/128 LABELS\n",
      "DONE 91/128 LABELS\n",
      "DONE 92/128 LABELS\n",
      "DONE 93/128 LABELS\n",
      "DONE 94/128 LABELS\n",
      "DONE 95/128 LABELS\n",
      "DONE 96/128 LABELS\n",
      "DONE 97/128 LABELS\n",
      "DONE 98/128 LABELS\n",
      "DONE 99/128 LABELS\n",
      "DONE 100/128 LABELS\n",
      "DONE 101/128 LABELS\n",
      "DONE 102/128 LABELS\n",
      "DONE 103/128 LABELS\n",
      "DONE 104/128 LABELS\n",
      "DONE 105/128 LABELS\n",
      "DONE 106/128 LABELS\n",
      "DONE 107/128 LABELS\n",
      "DONE 108/128 LABELS\n",
      "DONE 109/128 LABELS\n",
      "DONE 110/128 LABELS\n",
      "DONE 111/128 LABELS\n",
      "DONE 112/128 LABELS\n",
      "DONE 113/128 LABELS\n",
      "DONE 114/128 LABELS\n",
      "DONE 115/128 LABELS\n",
      "DONE 116/128 LABELS\n",
      "DONE 117/128 LABELS\n",
      "DONE 118/128 LABELS\n",
      "DONE 119/128 LABELS\n",
      "DONE 120/128 LABELS\n",
      "DONE 121/128 LABELS\n",
      "DONE 122/128 LABELS\n",
      "DONE 123/128 LABELS\n",
      "DONE 124/128 LABELS\n",
      "DONE 125/128 LABELS\n",
      "DONE 126/128 LABELS\n",
      "DONE 127/128 LABELS\n",
      "DONE 128/128 LABELS\n",
      "2019-10-19 14:48:58,978 Reading data from weighted_sampling\n",
      "2019-10-19 14:48:58,980 Train: weighted_sampling/labelled_128_1571496538.csv\n",
      "2019-10-19 14:48:58,981 Dev: weighted_sampling/valid.csv\n",
      "2019-10-19 14:48:58,981 Test: weighted_sampling/test.csv\n",
      "2019-10-19 14:48:59,025 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:48:59,026 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-19 14:48:59,027 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:48:59,028 Corpus: \"Corpus: 128 train + 10001 dev + 10001 test sentences\"\n",
      "2019-10-19 14:48:59,028 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:48:59,029 Parameters:\n",
      "2019-10-19 14:48:59,030  - learning_rate: \"1e-05\"\n",
      "2019-10-19 14:48:59,031  - mini_batch_size: \"32\"\n",
      "2019-10-19 14:48:59,034  - patience: \"5\"\n",
      "2019-10-19 14:48:59,034  - anneal_factor: \"0.5\"\n",
      "2019-10-19 14:48:59,035  - max_epochs: \"1\"\n",
      "2019-10-19 14:48:59,037  - shuffle: \"True\"\n",
      "2019-10-19 14:48:59,038  - train_with_dev: \"False\"\n",
      "2019-10-19 14:48:59,039 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:48:59,039 Model training base path: \"weighted_sampling/18\"\n",
      "2019-10-19 14:48:59,040 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:48:59,041 Device: cuda:0\n",
      "2019-10-19 14:48:59,041 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:48:59,042 Embeddings storage mode: gpu\n",
      "2019-10-19 14:48:59,047 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:48:59,716 epoch 1 - iter 0/4 - loss 0.32060194 - samples/sec: 144.50\n",
      "2019-10-19 14:48:59,943 epoch 1 - iter 1/4 - loss 0.30426972 - samples/sec: 153.58\n",
      "2019-10-19 14:49:00,172 epoch 1 - iter 2/4 - loss 0.29404434 - samples/sec: 153.61\n",
      "2019-10-19 14:49:00,394 epoch 1 - iter 3/4 - loss 0.27301297 - samples/sec: 156.95\n",
      "2019-10-19 14:49:00,639 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:49:00,640 EPOCH 1 done: loss 0.2730 - lr 0.0000\n",
      "2019-10-19 14:50:08,634 DEV : loss 2.6007580757141113 - score 0.5087\n",
      "2019-10-19 14:50:10,813 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kunwar31/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19 14:50:17,555 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-19 14:50:17,556 Testing using best model ...\n",
      "2019-10-19 14:50:17,557 loading file weighted_sampling/18/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "for step_num in range(1, 100):\n",
    "    classifier, opt_state = learner.step(classifier, step_num=step_num, optimizer_state=opt_state, max_sample_size=100000, labelling_step_size=128, step_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
